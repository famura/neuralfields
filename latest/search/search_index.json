{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Overview","text":""},{"location":"#neural-fields-old-idea-new-glory","title":"Neural Fields \u2013 Old Idea, New Glory","text":""},{"location":"#about","title":"About","text":"<p>In 1977, Shun-ichi Amari introduced neural fields, a class of potential-based recurrent neural networks [1]. This architecture was developed as a simplistic model of the activity of neurons in a (human) brain. It's main characteristic is the lateral in-/exhibition of neurons though their accumulated potential. Due to its simplicity and expressiveness, Amari\u2019s work was highly influential and led to several follow-up papers such as [2-6] to only name a few.</p>"},{"location":"#features","title":"Features","text":"<ul> <li>There are two variants of the neural fields implemented in this repository: one called <code>NeuralField</code> that matches   the model of Amari closely using 1D convolutions, as well as another one called <code>SimpleNeuralField</code> that replaces the   convolutions and introduces custom potential dynamics function.</li> <li>Both implementations have by modern standards very few, i.e., typically less than 1000, parameters. I suggest that you   start with the <code>NeuralField</code> class since it is more expressive. However, the <code>SimpleNeuralField</code> has the benefit of   operating with typically less than 20 parameters, which allows you to use optimizers that otherwise might not scale.</li> <li>Both, <code>NeuralField</code> and <code>SimpleNeuralField</code>, model classes are subclasses of <code>torch.nn.Module</code>, hence able to process   batched data and run on GPUs.</li> <li>The examples contain a script for time series learning.   However, it is also possible to use neural fields as generative models.</li> <li>This repository is a spin-off from SimuRLacra where the neural fields have   been used as the backbone for control policies. In <code>SimuRLacra</code>, the focus is on reinforcement learning for   sim-to-real transfer. However, the goal of this repository is to make the implementation as general as possible,   such that it could for example be used as generative model.</li> </ul>"},{"location":"#citing","title":"Citing","text":"<p>If you use code or ideas from this repository for your projects or research, please cite and star it. It does not cost you anything, and would support me for putting in the effort of providing a clean state-of-the-art implementation to you.</p> <pre><code>@misc{Muratore_neuralfields,\n  author = {Fabio Muratore},\n  title = {neuralfields - A type of potential-based recurrent neural networks implemented with PyTorch},\n  year = {2023},\n  publisher = {GitHub},\n  journal = {GitHub repository},\n  howpublished = {\\url{https://github.com/famura/neuralfields}}\n}\n</code></pre>"},{"location":"#getting-started","title":"Getting Started","text":"<p>To install this package, simply run</p> <pre><code>pip install neuralfields\n</code></pre> <p>For further information, please have a look at the getting started guide. In the documentation, you can also find the complete reference of the source code.</p>"},{"location":"#references","title":"References","text":"<p>[1] S-I. Amari. Dynamics of pattern formation in lateral-inhibition type neural fields. Biological Cybernetics. 1977. [2] K. Kishimoto and S-I. Amari. Existence and stability of local excitations in homogeneous neural fields. Journal of Mathematical Biology, 1979. [3] W. Erlhagen and G. Sch\u00f6ner. Dynamic field theory of movement preparation. Psychological Review, 2002. [4] S-I. Amari, H. Park, and T. Ozeki. Singularities affect dynamics of learning in neuromanifolds. Neural Computation, 2006. [5] T. Luksch, M. Gineger, M. M\u00fchlig, T. Yoshiike, Adaptive Movement Sequences and Predictive Decisions based on Hierarchical Dynamical Systems. International Conference on Intelligent Robots and Systems, 2012. [6] C. Kuehn and  J. M. T\u00f6lle. A gradient flow formulation for the stochastic Amari neural field model. Journal of Mathematical Biology, 2019.</p>"},{"location":"contributing/","title":"Contributing","text":"<p>Contributions are always welcome and can be done is multiple ways, e.g. pull requests, bug reports, code review, or feedback of any kind. Speak freely but respectful.</p> <p>Please do my OCD a favor and follow this naming convention for branches: <code>&lt;classifier&gt;/&lt;description_with_underscores&gt;</code>, e.g. <code>fix/missing_import</code>, or <code>feat/cuda_support</code>, and this convention for commit messages: <code>&lt;Start with capital letter and not dot at the end&gt;</code>, e.g. <code>Added new feature</code> or <code>Fixed typo</code>.</p>"},{"location":"getting_started/","title":"Getting Started","text":""},{"location":"getting_started/#getting-started-with-neuralfields","title":"Getting started with <code>neuralfields</code>","text":"<p>This pages explains how to install <code>neuralfields</code> via the package management system <code>poetry</code>, how the CI/CD pipelines are set up using <code>poe</code> tasks, and how <code>pre-commit</code> hooks are configures to keep the commits clean.</p>"},{"location":"getting_started/#installation","title":"Installation","text":""},{"location":"getting_started/#installation-of-poetry","title":"Installation of <code>poetry</code>","text":"<p>This project is managed by poetry, a Python packaging and dependency management tool. Therefore, <code>poetry</code> must be installed first.</p> <p>Please have a look at the official poetry documentation on how to install <code>poetry</code> on different platforms and under different conditions.</p>"},{"location":"getting_started/#installation-of-neuralfields","title":"Installation of <code>neuralfields</code>","text":"<p>The installation of this project is quite straightforward. Simply go to the project's directory, and run</p> <pre><code>poetry install\n</code></pre> <p>No project development intended?</p> <p>If you don't need any development setup, you can pass the <code>--no-dev</code> flag to skip the development dependencies.</p> Computer says no... <p>Please find a collection of known failure cases below, and feel free to report more.</p> Symptom Hint Something is wrong with my poetry environment Delete the <code>.venv</code> folder and recreate the virtual environment."},{"location":"getting_started/#dependency-management-packaging","title":"Dependency Management &amp; Packaging","text":"<p>As mentioned in the Installation section, poetry is employed to keep the dependencies of different projects from interfering with each other. By running <code>poetry install</code> in the project's root folder, a separate virtual environment is created into which all dependencies are installed automatically (typically takes a few seconds to minutes). This is similar to running <code>pip install -r requirements.txt</code> in an isolated virtual environment.</p>"},{"location":"getting_started/#poe-task-runner","title":"Poe Task Runner","text":"<p>This project defines so-called tasks using poe which are executed in commit hooks as well as the CI/CD pipeline. These tasks are essentially a sequence of commands, and can also be executed locally in the terminal by running</p> <pre><code>poetry run poe &lt;task_name&gt;\n</code></pre> Available tasks <p>To get a list of available tasks, execute <code>poetry run poe --help</code></p> <pre><code>Poe the Poet - A task runner that works well with poetry.\nversion 0.17.1\n\nUSAGE\n  poe [-h] [-v | -q] [--root PATH] [--ansi | --no-ansi] task [task arguments]\n\nGLOBAL OPTIONS\n  -h, --help     Show this help page and exit\n--version      Print the version and exit\n-v, --verbose  Increase command output (repeatable)\n-q, --quiet    Decrease command output (repeatable)\n-d, --dry-run  Print the task contents but don't actually run it\n  --root PATH    Specify where to find the pyproject.toml\n  --ansi         Force enable ANSI output\n  --no-ansi      Force disable ANSI output\n\nCONFIGURED TASKS\n  bump-version-tag         Bump version. This creates a new git tag based on the desired version part. Note that this task does not\nactually push the tag. You can do this manually, e.g. by running 'poe push-latest-version-tag'.\n    part                   Part of version being bumped. Allowed values: patch, minor, major.\n    --release              Wether this is a release. Then, the tag will be annotated.\n  clean                    Clean up all temporary files.\n  format                   Format or check Python files with black, isort, pyupgrade &amp; autoflake.\n    --check                If true, only check if the files are formatted but do not format them.\n    files                  List of files (optional).\n  lint                     Lint Python files with mypy, pylint, and bandit. The reports are stored in the given directory.\n    files                  List of files or directories (optional).\n    --reportdir            Diretory to write the linters' reports to (optional).\n  test                     Run the project's tests using pytest (with --exitfirst). Then compute the test coverage and compile it to html.\n  docs                     Build the docs (needs completed test task).\n  deploy-docs              Deploy the docs (needs completed docs task).\n    --alias                Version alias.\n    --push                 Wether to push the docs to GitHub pages.\n    --version-postfix      Information appended to version (optional).\n  deploy-package           Deploy package to PyPI (no --repository PRIVATE_REPO is needed).\n    --username             Repository user name.\n    --password             Repository password / access token.\n  push-latest-version-tag  Push the latest version tag.\n  release                  Make a new (stable) release. This will test the package, create a new tag based on the version, build and\ndeploy the docs, and finally push the new tag to remote.\n    part                   Release type. Allowed values: patch, minor, major.\n    --password             The repository password / access token.\n    --username             The repository user name.\n</code></pre>"},{"location":"getting_started/#git-hooks","title":"Git Hooks","text":"<p>This project uses pre-commit hooks to automatically check for common formatting issues. The hooks are executed before every commit, but can be disabled by adding <code>--no-verify</code> when committing, e.g. <code>git commit . -m \"Fixed something\" --no-verify</code>.</p> <p>Installation of pre-commit</p> <p>After you cloned this project and plan to develop in it, don't forget to install these hooks via</p> <pre><code>poetry run pre-commit install\n</code></pre> Available pre-commit hooks <p>The pre-commit hooks are configured in the <code>.pre-commit-config.yaml</code> file as follows</p> <pre><code>repos:\n- repo: https://github.com/pre-commit/pre-commit-hooks\nrev: v4.4.0\nhooks:\n- id: check-added-large-files\nargs: [--maxkb=5000] # limit to 5MB per file\n- id: check-toml\n- id: check-yaml\nargs: [--unsafe] # instead of loading the files, simply parse them for syntax\n- id: destroyed-symlinks\n- id: detect-private-key\n- id: end-of-file-fixer\n- id: fix-byte-order-marker\n- id: mixed-line-ending\n- id: name-tests-test\nargs: [--pytest-test-first]\n- id: trailing-whitespace\n- repo: https://github.com/psf/black\nrev: 23.1.0\nhooks:\n- id: black\n- repo: https://github.com/pycqa/isort\nrev: 5.12.0\nhooks:\n- id: isort\n</code></pre>"},{"location":"getting_started/#github-actions","title":"GitHub Actions","text":"<p>There are basic CI, CD, and Release pipelines which are executed as GitHub actions workflow on pushing changes or opening pull requests.</p> Available workflows .github/workflows/ci.yaml.github/workflows/cd.yaml.github/workflows/release.yaml <pre><code>name: Continuous Integration\n\non: [pull_request, push, workflow_dispatch]\n\ndefaults:\nrun:\nshell: bash\n\nconcurrency:\ngroup: ${{ github.workflow }}-${{ github.head_ref || github.ref }}\ncancel-in-progress: true\n\njobs:\nci:\nname: CI\nstrategy:\nmatrix:\nos: [ubuntu-latest] # [ubuntu-latest, macos-latest, windows-latest]\npython-version: [ \"3.8\", \"3.10\", \"3.11\" ]\nruns-on: ${{ matrix.os }}\nenv:\nos: ${{ matrix.os }}\npython: ${{ matrix.python-version }}\n\nsteps:\n- name: Install prerequisites\nenv:\nDEBIAN_FRONTEND: noninteractive\nrun: |\nsudo apt-get update\nsudo apt-get install --yes curl gcc git\n\n- name: Set up Python ${{ matrix.python-version }}\nuses: actions/setup-python@v4\nwith:\npython-version: ${{ matrix.python-version }}\n\n- name: Set up poetry\nrun: |\ncurl -sSL https://install.python-poetry.org | python3 - --force\necho \"$HOME/.local/bin\" &gt;&gt; $GITHUB_PATH\n\n- name: Checkout repository\nuses: actions/checkout@v3\nwith:\nfetch-depth: 0\n\n- name: Configure git\nrun: |\ngit config user.name \"${GITHUB_ACTOR}\"\ngit config user.email \"${GITHUB_ACTOR}@users.noreply.github.com\"\n\n- name: Install dependencies\nrun: poetry install\n\n- name: Install and run pre-commit hooks\nenv:\nSKIP: no-commit-to-branch\nrun: |\npoetry run pre-commit install\npoetry run pre-commit run --all-files --show-diff-on-failure\n\n- name: Run tests\nrun: poetry run poe test\n\n- name: Code Coverage Summary\nuses: irongut/CodeCoverageSummary@v1.3.0\nwith:\nfilename: coverage.xml\nformat: markdown\noutput: both\n\n- name: Build docs\nrun: poetry run poe docs\n\n- name: Deploy temporary docs\nif: github.event_name == 'pull_request'\nrun: |\npoetry run poe deploy-docs \\\n--push \\\n--alias pr-${{ github.event.number }} \\\n--version-postfix pr-${{ github.event.number }}\n\n- name: Write PR note\nif: github.event_name == 'pull_request'\nrun: |\ncat &lt;&lt;EOT&gt;&gt; pr_ci_note.md\n\n[temporary docs](https://github.com/famura/neuralfields/pr-${{ github.event.number }}\").\n[tests](https://github.com/famura/neuralfields/pr-${{ github.event.number }}/exported/tests/report.html)\n[coverage](https://github.com/famura/neuralfields/pr-${{ github.event.number }}/exported/coverage/report.html)\n\nEOT\n\n- name: Add PR note\nif: github.event_name == 'pull_request'\nuses: marocchino/sticky-pull-request-comment@v2.3.1\nwith:\npath: pr_ci_note.md\n</code></pre> <pre><code>name: Continuous Deployment\n\non:\npush:\nbranches: [main]\nworkflow_dispatch:\n\ndefaults:\nrun:\nshell: bash\n\nconcurrency:\ngroup: ${{ github.workflow }}-${{ github.head_ref || github.ref }}\ncancel-in-progress: true\n\njobs:\ncd:\nname: CD\nif: github.repository == 'famura/neuralfields'\nruns-on: ubuntu-latest\npermissions: # https://docs.github.com/en/actions/security-guides/automatic-token-authentication\ncontents: write # to publish the docs to gh-pages\nenv:\nos: ubuntu-latest\npython: \"3.10\"\n\nsteps:\n- name: Install prerequisites\nenv:\nDEBIAN_FRONTEND: noninteractive\nrun: |\nsudo apt-get update\nsudo apt-get install --yes curl gcc git\n\n- name: Set up Python 3.10\nuses: actions/setup-python@v4\nwith:\npython-version: \"3.10\"\n\n- name: Set up poetry\nrun: |\ncurl -sSL https://install.python-poetry.org | python3 - --force\necho \"$HOME/.local/bin\" &gt;&gt; $GITHUB_PATH\n\n- name: Checkout repository\nuses: actions/checkout@v3\nwith:\nfetch-depth: 0\n\n- name: Configure git\nrun: |\ngit config user.name \"${GITHUB_ACTOR}\"\ngit config user.email \"${GITHUB_ACTOR}@users.noreply.github.com\"\n\n- name: Install dependencies\nrun: poetry install\n\n- name: Bump patch version\nrun: poetry run poe bump-version-tag patch\n\n- name: Deploy docs\nenv:\nGITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}\nrun: |\npoetry run poe test\npoetry run poe docs\npoetry run poe deploy-docs --push --alias latest\n\n- name: Deploy package\nrun: |\npoetry run poe deploy-package --username ${{ secrets.PYPI_USER }} --password ${{ secrets.PYPI_TOKEN }}\n\n- name: Push version tag\nrun: poetry run poe push-latest-version-tag\n\nrelease_draft:\nname: Update release notes\nruns-on: ubuntu-latest\npermissions: # https://docs.github.com/en/actions/security-guides/automatic-token-authentication\ncontents: write\n\nsteps:\n- uses: release-drafter/release-drafter@v5.22.0\nwith:\nconfig-name: release_drafter.yaml\nenv:\nGITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}\n</code></pre> <pre><code>name: Release\n\non:\nworkflow_dispatch:\ninputs:\nbumped-version-part:\ndescription: \"The version part to bump.\"\ntype: choice\noptions:\n- major\n- minor\n- patch\ndefault: \"minor\"\nrequired: true\n\ndefaults:\nrun:\nshell: bash\n\nconcurrency:\ngroup: ${{ github.workflow }}-${{ github.head_ref || github.ref }}\ncancel-in-progress: true\n\njobs:\nrelease:\nname: Release\nif: github.repository == 'famura/neuralfields'\nruns-on: ubuntu-latest\npermissions: # https://docs.github.com/en/actions/security-guides/automatic-token-authentication\ncontents: write # to publish the docs to gh-pages\nenv:\nos: ubuntu-latest\npython: \"3.10\"\n\nsteps:\n- name: Install prerequisites\nenv:\nDEBIAN_FRONTEND: noninteractive\nrun: |\nsudo apt-get update\nsudo apt-get install --yes curl gcc git\n\n- name: Set up Python 3.10\nuses: actions/setup-python@v4\nwith:\npython-version: \"3.10\"\n\n- name: Set up poetry\nrun: |\ncurl -sSL https://install.python-poetry.org | python3 - --force\necho \"$HOME/.local/bin\" &gt;&gt; $GITHUB_PATH\n\n- name: Checkout repository\nuses: actions/checkout@v3\nwith:\nfetch-depth: 0\n\n- name: Configure git\nrun: |\ngit config user.name \"${GITHUB_ACTOR}\"\ngit config user.email \"${GITHUB_ACTOR}@users.noreply.github.com\"\n\n- name: Install dependencies\nrun: poetry install\n\n- name: Publish to PyPI\nrun: |\npoetry run poe release ${{ github.event.inputs.bumped-version-part }} \\\n--username ${{ secrets.PYPI_USER }} \\\n--password ${{ secrets.PYPI_TOKEN }}\n</code></pre>"},{"location":"license/","title":"This Project","text":"<p>MIT License</p> <p>Copyright \u00a9 2023 Fabio Muratore</p> <p>Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:</p> <p>The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.</p> <p>THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.</p>"},{"location":"exported/changelog/","title":"Changelog","text":""},{"location":"exported/changelog/#v021-2023-02-25","title":"v0.2.1 (2023-02-25)","text":""},{"location":"exported/changelog/#v020-2023-02-24","title":"v0.2.0 (2023-02-24)","text":""},{"location":"exported/changelog/#v011-2023-02-24","title":"v0.1.1 (2023-02-24)","text":""},{"location":"exported/changelog/#v010-2023-02-23","title":"v0.1.0 (2023-02-23)","text":""},{"location":"exported/changelog/#v001-2023-02-23","title":"v0.0.1 (2023-02-23)","text":""},{"location":"exported/third_party_licenses/","title":"Third-Party Libraries","text":"Name Version License Author URL Description Babel 2.11.0 BSD License Armin Ronacher https://babel.pocoo.org/ Internationalization utilities GitPython 3.1.31 BSD License Sebastian Thiel, Michael Trier https://github.com/gitpython-developers/GitPython GitPython is a Python library used to interact with Git repositories Jinja2 3.1.2 BSD License Armin Ronacher https://palletsprojects.com/p/jinja/ A very fast and expressive template engine. Markdown 3.3.7 BSD License Manfred Stienstra, Yuri takhteyev and Waylan limberg https://Python-Markdown.github.io/ Python implementation of Markdown. MarkupSafe 2.1.2 BSD License Armin Ronacher https://palletsprojects.com/p/markupsafe/ Safely add untrusted strings to HTML/XML markup. Pillow 9.4.0 Historical Permission Notice and Disclaimer (HPND) Alex Clark (PIL Fork Author) https://python-pillow.org Python Imaging Library (Fork) PyQt5 5.15.9 GPL v3 Riverbank Computing Limited https://www.riverbankcomputing.com/software/pyqt/ Python bindings for the Qt cross platform application toolkit PyQt5-Qt5 5.15.2 LGPL v3 Riverbank Computing Limited https://www.riverbankcomputing.com/software/pyqt/ The subset of a Qt installation needed by PyQt5. PyQt5-sip 12.11.1 SIP Riverbank Computing Limited https://www.riverbankcomputing.com/software/sip/ The sip module support for PyQt5 PyYAML 6.0 MIT License Kirill Simonov https://pyyaml.org/ YAML parser and emitter for Python Pygments 2.14.0 BSD License Georg Brandl https://pygments.org/ Pygments is a syntax highlighting package written in Python. astroid 2.14.2 GNU Lesser General Public License v2 (LGPLv2) UNKNOWN UNKNOWN An abstract syntax tree for Python with inference support. attrs 22.2.0 MIT License Hynek Schlawack https://www.attrs.org/ Classes Without Boilerplate autoflake 2.0.1 MIT License UNKNOWN UNKNOWN Removes unused imports and unused variables bandit 1.7.4 Apache Software License PyCQA https://bandit.readthedocs.io/ Security oriented static analyser for python code. black 22.12.0 MIT License UNKNOWN UNKNOWN The uncompromising code formatter. certifi 2022.12.7 Mozilla Public License 2.0 (MPL 2.0) Kenneth Reitz https://github.com/certifi/python-certifi Python package for providing Mozilla's CA Bundle. cfgv 3.3.1 MIT License Anthony Sottile https://github.com/asottile/cfgv Validate configuration and produce human readable error messages. charset-normalizer 3.0.1 MIT License Ahmed TAHRI https://github.com/Ousret/charset_normalizer The Real First Universal Charset Detector. Open, modern and actively maintained alternative to Chardet. click 8.1.3 BSD License Armin Ronacher https://palletsprojects.com/p/click/ Composable command line interface toolkit colorama 0.4.6 BSD License UNKNOWN UNKNOWN Cross-platform colored terminal text. contourpy 1.0.7 BSD License Ian Thomas UNKNOWN Python library for calculating contours of 2D quadrilateral grids coverage 7.1.0 Apache Software License Ned Batchelder and 166 others https://github.com/nedbat/coveragepy Code coverage measurement for Python csscompressor 0.9.5 BSD License Yury Selivanov http://github.com/sprymix/csscompressor A python port of YUI CSS Compressor cycler 0.11.0 BSD License Thomas A Caswell https://github.com/matplotlib/cycler Composable style cycles defusedxml 0.7.1 Python Software Foundation License Christian Heimes https://github.com/tiran/defusedxml XML bomb protection for Python stdlib modules dill 0.3.6 BSD License Mike McKerns https://github.com/uqfoundation/dill serialize all of python distlib 0.3.6 Python Software Foundation License Vinay Sajip https://github.com/pypa/distlib Distribution utilities dunamai 1.15.0 MIT License Matthew T. Kennerly https://github.com/mtkennerly/dunamai Dynamic version generation exceptiongroup 1.1.0 MIT License UNKNOWN UNKNOWN Backport of PEP 654 (exception groups) filelock 3.9.0 The Unlicense (Unlicense) UNKNOWN UNKNOWN A platform independent file lock. fonttools 4.38.0 MIT License Just van Rossum http://github.com/fonttools/fonttools Tools to manipulate font files genbadge 1.1.0 BSD License Sylvain MARIE sylvain.marie@se.com https://github.com/smarie/python-genbadge Generate badges for tools that do not provide one. ghp-import 2.1.0 Apache Software License Paul Joseph Davis https://github.com/c-w/ghp-import Copy your docs directly to the gh-pages branch. git-changelog 0.6.0 ISC UNKNOWN UNKNOWN Automatic Changelog generator using Jinja2 templates. gitdb 4.0.10 BSD License Sebastian Thiel https://github.com/gitpython-developers/gitdb Git Object Database griffe 0.25.5 ISC UNKNOWN UNKNOWN Signatures for entire Python programs. Extract the structure, the frame, the skeleton of your project, to generate API documentation or find breaking changes in your API. htmlmin 0.1.12 BSD License Dave Mankoff https://htmlmin.readthedocs.io/en/latest/ An HTML Minifier identify 2.5.18 MIT License Chris Kuehl https://github.com/pre-commit/identify File identification library for Python idna 3.4 BSD License UNKNOWN UNKNOWN Internationalized Domain Names in Applications (IDNA) importlib-metadata 6.0.0 Apache Software License Jason R. Coombs https://github.com/python/importlib_metadata Read metadata from Python packages iniconfig 2.0.0 MIT License UNKNOWN UNKNOWN brain-dead simple config-ini parsing isort 5.12.0 MIT License Timothy Crosley https://pycqa.github.io/isort/ A Python utility / library to sort Python imports. jsmin 3.0.1 MIT License Dave St.Germain https://github.com/tikitu/jsmin/ JavaScript minifier. kiwisolver 1.4.4 BSD License UNKNOWN UNKNOWN A fast implementation of the Cassowary constraint solver lazy-object-proxy 1.9.0 BSD License Ionel Cristian M\u0103rie\u0219 https://github.com/ionelmc/python-lazy-object-proxy A fast and thorough lazy object proxy. lxml 4.9.2 BSD License lxml dev team https://lxml.de/ Powerful and Pythonic XML processing library combining libxml2/libxslt with the ElementTree API. matplotlib 3.7.0 Python Software Foundation License John D. Hunter, Michael Droettboom https://matplotlib.org Python plotting package mccabe 0.7.0 MIT License Tarek Ziade https://github.com/pycqa/mccabe McCabe checker, plugin for flake8 mergedeep 1.3.4 MIT License Travis Clarke https://github.com/clarketm/mergedeep A deep merge function for \ud83d\udc0d. mike 1.1.2 BSD License Jim Porter https://github.com/jimporter/mike Manage multiple versions of your MkDocs-powered documentation mkdocs 1.4.2 BSD License UNKNOWN UNKNOWN Project documentation with Markdown. mkdocs-autorefs 0.4.1 ISC License (ISCL) UNKNOWN UNKNOWN Automatically link across pages in MkDocs. mkdocs-gen-files 0.4.0 MIT License Oleh Prypin https://github.com/oprypin/mkdocs-gen-files MkDocs plugin to programmatically generate documentation pages during the build mkdocs-git-revision-date-localized-plugin 1.1.0 MIT License Tim Vink https://github.com/timvink/mkdocs-git-revision-date-localized-plugin Mkdocs plugin that enables displaying the localized date of the last git modification of a markdown file. mkdocs-literate-nav 0.5.0 MIT License Oleh Prypin https://github.com/oprypin/mkdocs-literate-nav MkDocs plugin to specify the navigation in Markdown instead of YAML mkdocs-material 9.0.13 MIT License UNKNOWN UNKNOWN Documentation that simply works mkdocs-material-extensions 1.1.1 MIT License UNKNOWN UNKNOWN Extension pack for Python Markdown and MkDocs Material. mkdocs-minify-plugin 0.6.2 MIT License Byrne Reese, Lars Wilhelmer https://github.com/byrnereese/mkdocs-minify-plugin An MkDocs plugin to minify HTML, JS or CSS files prior to being written to disk mkdocstrings 0.20.0 ISC UNKNOWN UNKNOWN Automatic documentation from sources, for MkDocs. mkdocstrings-python 0.8.3 ISC UNKNOWN UNKNOWN A Python handler for mkdocstrings. mypy 0.991 MIT License Jukka Lehtosalo http://www.mypy-lang.org/ Optional static typing for Python mypy-extensions 1.0.0 MIT License The mypy developers https://github.com/python/mypy_extensions Type system extensions for programs checked with the mypy type checker. neuralfields 0.0.0 MIT License Fabio Muratore https://github.com/famura/neuralfields A type of potential-based recurrent neural networks implemented with PyTorch nodeenv 1.7.0 BSD License Eugene Kalinin https://github.com/ekalinin/nodeenv Node.js virtual environment builder numpy 1.24.2 BSD License Travis E. Oliphant et al. https://www.numpy.org Fundamental package for array computing in Python nvidia-cublas-cu11 11.10.3.66 Other/Proprietary License Nvidia CUDA Installer Team https://developer.nvidia.com/cuda-zone CUBLAS native runtime libraries nvidia-cuda-nvrtc-cu11 11.7.99 Other/Proprietary License Nvidia CUDA Installer Team https://developer.nvidia.com/cuda-zone NVRTC native runtime libraries nvidia-cuda-runtime-cu11 11.7.99 Other/Proprietary License Nvidia CUDA Installer Team https://developer.nvidia.com/cuda-zone CUDA Runtime native Libraries nvidia-cudnn-cu11 8.5.0.96 Other/Proprietary License Nvidia CUDA Installer Team https://developer.nvidia.com/cuda-zone cuDNN runtime libraries packaging 23.0 Apache Software License; BSD License UNKNOWN UNKNOWN Core utilities for Python packages pandas 1.5.3 BSD License The Pandas Development Team https://pandas.pydata.org Powerful data structures for data analysis, time series, and statistics pastel 0.2.1 MIT License S\u00e9bastien Eustace https://github.com/sdispater/pastel Bring colors to your terminal. pathspec 0.11.0 Mozilla Public License 2.0 (MPL 2.0) UNKNOWN UNKNOWN Utility library for gitignore style pattern matching of file paths. pbr 5.11.1 Apache Software License OpenStack https://docs.openstack.org/pbr/latest/ Python Build Reasonableness platformdirs 3.0.0 MIT License UNKNOWN UNKNOWN A small Python package for determining appropriate platform-specific dirs, e.g. a \"user data dir\". pluggy 1.0.0 MIT License Holger Krekel https://github.com/pytest-dev/pluggy plugin and hook calling mechanisms for python poethepoet 0.17.1 MIT License Nat Noordanus https://github.com/nat-n/poethepoet A task runner that works well with poetry. pre-commit 2.21.0 MIT License Anthony Sottile https://github.com/pre-commit/pre-commit A framework for managing and maintaining multi-language pre-commit hooks. py 1.11.0 MIT License holger krekel, Ronny Pfannschmidt, Benjamin Peterson and others https://py.readthedocs.io/ library with cross-python path, ini-parsing, io, code, log facilities pyflakes 3.0.1 MIT License A lot of people https://github.com/PyCQA/pyflakes passive checker of Python programs pylint 2.16.2 GNU General Public License v2 (GPLv2) UNKNOWN UNKNOWN python code static checker pymdown-extensions 9.9.2 MIT License UNKNOWN UNKNOWN Extension pack for Python Markdown. pyparsing 3.0.9 MIT License UNKNOWN UNKNOWN pyparsing module - Classes and methods to define and execute parsing grammars pytest 7.2.1 MIT License Holger Krekel, Bruno Oliveira, Ronny Pfannschmidt, Floris Bruynooghe, Brianna Laugher, Florian Bruhin and others https://docs.pytest.org/en/latest/ pytest: simple powerful testing with Python pytest-cov 4.0.0 MIT License Marc Schlaich https://github.com/pytest-dev/pytest-cov Pytest plugin for measuring coverage. pytest-html 3.2.0 Mozilla Public License 2.0 (MPL 2.0) Dave Hunt https://github.com/pytest-dev/pytest-html pytest plugin for generating HTML reports pytest-lazy-fixture 0.6.3 MIT License Marsel Zaripov https://github.com/tvorog/pytest-lazy-fixture It helps to use fixtures in pytest.mark.parametrize pytest-metadata 2.0.4 MPL-2.0 Dave Hunt https://github.com/pytest-dev/pytest-metadata pytest plugin for test session metadata python-dateutil 2.8.2 Apache Software License; BSD License Gustavo Niemeyer https://github.com/dateutil/dateutil Extensions to the standard Python datetime module pytz 2022.7.1 MIT License Stuart Bishop http://pythonhosted.org/pytz World timezone definitions, modern and historical pyupgrade 3.3.1 MIT License Anthony Sottile https://github.com/asottile/pyupgrade A tool to automatically upgrade syntax for newer versions. pyyaml_env_tag 0.1 MIT License Waylan Limberg https://github.com/waylan/pyyaml-env-tag A custom YAML tag for referencing environment variables in YAML files. regex 2022.10.31 Apache Software License Matthew Barnett https://github.com/mrabarnett/mrab-regex Alternative regular expression module, to replace re. requests 2.28.2 Apache Software License Kenneth Reitz https://requests.readthedocs.io Python HTTP for Humans. seaborn 0.12.2 BSD License UNKNOWN UNKNOWN Statistical data visualization semver 2.13.0 BSD License Kostiantyn Rybnikov https://github.com/python-semver/python-semver Python helper for Semantic Versioning (http://semver.org/) six 1.16.0 MIT License Benjamin Peterson https://github.com/benjaminp/six Python 2 and 3 compatibility utilities smmap 5.0.0 BSD License Sebastian Thiel https://github.com/gitpython-developers/smmap A pure Python implementation of a sliding window memory map manager stevedore 5.0.0 Apache Software License OpenStack https://docs.openstack.org/stevedore/latest/ Manage dynamic plugins for Python applications tokenize-rt 5.0.0 MIT License Anthony Sottile https://github.com/asottile/tokenize-rt A wrapper around the stdlib <code>tokenize</code> which roundtrips. toml 0.10.2 MIT License William Pearson https://github.com/uiri/toml Python Library for Tom's Obvious, Minimal Language tomli 2.0.1 MIT License UNKNOWN UNKNOWN A lil' TOML parser tomlkit 0.11.6 MIT License S\u00e9bastien Eustace https://github.com/sdispater/tomlkit Style preserving TOML library torch 1.13.1 BSD License PyTorch Team https://pytorch.org/ Tensors and Dynamic neural networks in Python with strong GPU acceleration types-toml 0.10.8.4 Apache Software License UNKNOWN https://github.com/python/typeshed Typing stubs for toml typing_extensions 4.5.0 Python Software Foundation License UNKNOWN UNKNOWN Backported and Experimental Type Hints for Python 3.7+ urllib3 1.26.14 MIT License Andrey Petrov https://urllib3.readthedocs.io/ HTTP library with thread-safe connection pooling, file post, and more. verspec 0.1.0 Apache Software License; BSD License Jim Porter https://github.com/jimporter/verspec Flexible version handling virtualenv 20.19.0 MIT License UNKNOWN UNKNOWN Virtual Python Environment builder watchdog 2.2.1 Apache Software License Yesudeep Mangalapilly https://github.com/gorakhargosh/watchdog Filesystem events monitoring wrapt 1.14.1 BSD License Graham Dumpleton https://github.com/GrahamDumpleton/wrapt Module for decorators, wrappers and monkey patching. zipp 3.14.0 MIT License Jason R. Coombs https://github.com/jaraco/zipp Backport of pathlib-compatible object wrapper for zip files"},{"location":"reference/","title":"Code Reference","text":"<p>The structure of this reference (see navigation tree on the left) mimics the structure of the project's code base.</p>"},{"location":"reference/code_nav/","title":"Code nav","text":"<ul> <li>neuralfields</li> <li>custom_layers</li> <li>custom_types</li> <li>neural_fields</li> <li>potential_based</li> <li>simple_neural_fields</li> </ul>"},{"location":"reference/custom_layers/","title":"custom_layers","text":""},{"location":"reference/custom_layers/#neuralfields.custom_layers.IndependentNonlinearitiesLayer","title":"<code>IndependentNonlinearitiesLayer(in_features, nonlin, bias, weight=True)</code>","text":"<p>         Bases: <code>nn.Module</code></p> <p>Neural network layer to add a bias, multiply the result with a scaling factor, and then apply the given nonlinearity. If a list of nonlinearities is provided, every dimension will be processed separately. The scaling and the bias are learnable parameters.</p> <pre><code>nonlin: The nonlinear function to apply.\nbias: If `True`, a learnable bias is subtracted, else no bias is used.\nweight: If `True`, the input is multiplied with a learnable scaling factor.\n</code></pre> Source code in <code>neuralfields/custom_layers.py</code> <pre><code>def __init__(\n    self,\n    in_features: int,\n    nonlin: Union[ActivationFunction, Sequence[ActivationFunction]],\n    bias: bool,\n    weight: bool = True,\n):\n\"\"\"\n    Args:\n        in_features: Number of dimensions of each input sample.\n        nonlin: The nonlinear function to apply.\n        bias: If `True`, a learnable bias is subtracted, else no bias is used.\n        weight: If `True`, the input is multiplied with a learnable scaling factor.\n    \"\"\"\n    if not callable(nonlin):\n        if len(nonlin) != in_features:\n            raise RuntimeError(\n                f\"Either one, or {in_features} nonlinear functions have been expected, but \"\n                f\"{len(nonlin)} have been given!\"\n            )\n\n    super().__init__()\n\n    # Create and initialize the parameters, and the activation function.\n    self.nonlin = copy.deepcopy(nonlin) if _is_iterable(nonlin) else nonlin\n    if weight:\n        self.weight = nn.Parameter(torch.empty(in_features, dtype=torch.get_default_dtype()), requires_grad=True)\n    else:\n        self.weight = None\n    if bias:\n        self.bias = nn.Parameter(torch.empty(in_features, dtype=torch.get_default_dtype()), requires_grad=True)\n    else:\n        self.bias = None\n    init_param_(self)\n</code></pre>"},{"location":"reference/custom_layers/#neuralfields.custom_layers.IndependentNonlinearitiesLayer.forward","title":"<code>forward(inp)</code>","text":"<p>Apply a bias, scaling, and a nonliterary to each input separately.</p> <p>\\(y = f_{nlin}( w * (x + b) )\\)</p> <p>Parameters:</p> Name Type Description Default <code>inp</code> <code>torch.Tensor</code> <p>Arbitrary input tensor.</p> required <p>Returns:</p> Type Description <code>torch.Tensor</code> <p>Output tensor.</p> Source code in <code>neuralfields/custom_layers.py</code> <pre><code>def forward(self, inp: torch.Tensor) -&gt; torch.Tensor:\n\"\"\"Apply a bias, scaling, and a nonliterary to each input separately.\n\n    $y = f_{nlin}( w * (x + b) )$\n\n    Args:\n        inp: Arbitrary input tensor.\n\n    Returns:\n        Output tensor.\n    \"\"\"\n    # Add bias if desired.\n    tmp = inp + self.bias if self.bias is not None else inp\n\n    # Apply weights if desired.\n    tmp = self.weight * tmp if self.weight is not None else tmp\n\n    # Every dimension runs through an individual nonlinearity.\n    if _is_iterable(self.nonlin):\n        return torch.tensor([fcn(tmp[idx]) for idx, fcn in enumerate(self.nonlin)])\n\n    # All dimensions identically.\n    return self.nonlin(tmp)  # type: ignore[operator]\n</code></pre>"},{"location":"reference/custom_layers/#neuralfields.custom_layers.MirroredConv1d","title":"<code>MirroredConv1d(in_channels, out_channels, kernel_size, stride=1, padding='same', dilation=1, groups=1, bias=False, padding_mode='zeros', device=None, dtype=None)</code>","text":"<p>         Bases: <code>_ConvNd</code></p> <p>A variant of the Conv1d module that re-uses parts of the convolution weights by mirroring the first half of the kernel (along the columns). This way we can save almost half of the parameters, under the assumption that we have a kernel that obeys this kind of symmetry. The biases are left unchanged.</p> Source code in <code>neuralfields/custom_layers.py</code> <pre><code>def __init__(\n    self,\n    in_channels: int,\n    out_channels: int,\n    kernel_size: int,\n    stride: int = 1,\n    padding: Union[int, str] = \"same\",  # kernel_size // 2 if padding_mode != \"circular\" else kernel_size - 1\n    dilation: int = 1,\n    groups: int = 1,\n    bias: bool = False,\n    padding_mode: str = \"zeros\",\n    device: Optional[Union[str, torch.device]] = None,\n    dtype=None,\n):\n    # Same as in PyTorch 1.12.\n    super().__init__(\n        in_channels=in_channels,\n        out_channels=out_channels,\n        kernel_size=_single(kernel_size),  # type: ignore[arg-type]\n        stride=_single(stride),  # type: ignore[arg-type]\n        padding=_single(padding) if not isinstance(padding, str) else padding,  # type: ignore[arg-type]\n        dilation=_single(dilation),  # type: ignore[arg-type]\n        transposed=False,\n        output_padding=_single(0),\n        groups=groups,\n        bias=bias,\n        padding_mode=padding_mode,\n        device=device,\n        dtype=dtype,\n    )\n\n    # Memorize PyTorch's weight shape (out_channels x in_channels x kernel_size) for later reconstruction.\n    self.orig_weight_shape = self.weight.shape\n\n    # Get number of kernel elements we later want to use for mirroring.\n    self.half_kernel_size = math.ceil(self.weight.size(2) / 2)  # kernel_size = 4 --&gt; 2, kernel_size = 5 --&gt; 3\n\n    # Initialize the weights values the same way PyTorch does.\n    new_weight_init = torch.zeros(self.orig_weight_shape[0], self.orig_weight_shape[1], self.half_kernel_size)\n    nn.init.kaiming_uniform_(new_weight_init, a=math.sqrt(5))\n\n    # Overwrite the weight attribute (transposed is False by default for the Conv1d module, we don't use it here).\n    self.weight = nn.Parameter(new_weight_init, requires_grad=True)\n</code></pre>"},{"location":"reference/custom_layers/#neuralfields.custom_layers.MirroredConv1d.forward","title":"<code>forward(inp)</code>","text":"<p>Computes the 1-dim convolution just like Conv1d, however, the kernel has mirrored weights, i.e., it is symmetric around its middle element, or in case of an even kernel size around an imaginary middle element.</p> <p>Parameters:</p> Name Type Description Default <code>inp</code> <code>torch.Tensor</code> <p>3-dim input tensor just like for Conv1d.</p> required <p>Returns:</p> Type Description <code>torch.Tensor</code> <p>3-dim output tensor just like for Conv1d.</p> Source code in <code>neuralfields/custom_layers.py</code> <pre><code>def forward(self, inp: torch.Tensor) -&gt; torch.Tensor:\n\"\"\"Computes the 1-dim convolution just like [Conv1d][torch.nn.Conv1d], however, the kernel has mirrored weights,\n    i.e., it is symmetric around its middle element, or in case of an even kernel size around an imaginary middle\n    element.\n\n    Args:\n        inp: 3-dim input tensor just like for [Conv1d][torch.nn.Conv1d].\n\n    Returns:\n        3-dim output tensor just like for [Conv1d][torch.nn.Conv1d].\n    \"\"\"\n    # Reconstruct symmetric weights for convolution (original size).\n    mirr_weight = torch.empty(self.orig_weight_shape, dtype=inp.dtype)\n\n    # Loop over input channels.\n    for i in range(self.orig_weight_shape[1]):\n        # Fill first half.\n        mirr_weight[:, i, : self.half_kernel_size] = self.weight[:, i, :]\n\n        # Fill second half (flip columns left-right).\n        if self.orig_weight_shape[2] % 2 == 1:\n            # Odd kernel size for convolution, don't flip the last column.\n            mirr_weight[:, i, self.half_kernel_size :] = torch.flip(self.weight[:, i, :], (1,))[:, 1:]\n        else:\n            # Even kernel size for convolution, flip all columns.\n            mirr_weight[:, i, self.half_kernel_size :] = torch.flip(self.weight[:, i, :], (1,))\n\n    # Run through the same function as the original PyTorch implementation, but with mirrored kernel.\n    return F.conv1d(\n        input=inp,\n        weight=mirr_weight,\n        bias=self.bias,\n        stride=self.stride,\n        padding=self.padding,\n        dilation=self.dilation,\n        groups=self.groups,\n    )\n</code></pre>"},{"location":"reference/custom_layers/#neuralfields.custom_layers.apply_bell_shaped_weights_conv_","title":"<code>apply_bell_shaped_weights_conv_(m, w, ks)</code>","text":"<p>Helper function to set the weights of a convolution layer according to a squared exponential.</p> <p>Parameters:</p> Name Type Description Default <code>m</code> <code>nn.Module</code> <p>Module containing the weights to be set.</p> required <code>w</code> <code>torch.Tensor</code> <p>Linearly spaced weights.</p> required <code>ks</code> <code>int</code> <p>Size of the convolution kernel.</p> required Source code in <code>neuralfields/custom_layers.py</code> <pre><code>@torch.no_grad()\ndef apply_bell_shaped_weights_conv_(m: nn.Module, w: torch.Tensor, ks: int) -&gt; None:\n\"\"\"Helper function to set the weights of a convolution layer according to a squared exponential.\n\n    Args:\n        m: Module containing the weights to be set.\n        w: Linearly spaced weights.\n        ks: Size of the convolution kernel.\n    \"\"\"\n    dim_ch_out, dim_ch_in = m.weight.data.size(0), m.weight.data.size(1)  # type: ignore[operator]\n    amp = torch.rand(dim_ch_out * dim_ch_in)\n    for i in range(dim_ch_out):\n        for j in range(dim_ch_in):\n            m.weight.data[i, j, :] = amp[i * dim_ch_in + j] * 2 * (torch.exp(-torch.pow(w, 2) / (ks / 2) ** 2) - 0.5)\n</code></pre>"},{"location":"reference/custom_layers/#neuralfields.custom_layers.init_param_","title":"<code>init_param_(m, **kwargs)</code>","text":"<p>Initialize the parameters of the PyTorch Module / layer / network / cell according to its type.</p> <p>Parameters:</p> Name Type Description Default <code>m</code> <code>torch.nn.Module</code> <p>Module containing the weights to be set.</p> required <code>kwargs</code> <code>Any</code> <p>Optional keyword arguments, e.g. <code>bell=True</code> to initialize a convolution layer's weight with a centered \"bell-shaped\" parameter value distribution.</p> <code>{}</code> Source code in <code>neuralfields/custom_layers.py</code> <pre><code>@torch.no_grad()\ndef init_param_(m: torch.nn.Module, **kwargs: Any) -&gt; None:\n\"\"\"Initialize the parameters of the PyTorch Module / layer / network / cell according to its type.\n\n    Args:\n        m: Module containing the weights to be set.\n        kwargs: Optional keyword arguments, e.g. `bell=True` to initialize a convolution layer's weight with a\n            centered \"bell-shaped\" parameter value distribution.\n    \"\"\"\n    kwargs = kwargs if kwargs is not None else dict()\n\n    if isinstance(m, nn.Conv1d):\n        if kwargs.get(\"bell\", False):\n            # Initialize the kernel weights with a shifted of shape exp(-x^2 / sigma^2).\n            # The biases are left unchanged.\n            if m.weight.data.size(2) % 2 == 0:\n                ks_half = m.weight.data.size(2) // 2\n                ls_half = torch.linspace(ks_half, 0, ks_half)  # descending\n                ls = torch.cat([ls_half, torch.flip(ls_half, (0,))])\n            else:\n                ks_half = math.ceil(m.weight.data.size(2) / 2)\n                ls_half = torch.linspace(ks_half, 0, ks_half)  # descending\n                ls = torch.cat([ls_half, torch.flip(ls_half[:-1], (0,))])\n            apply_bell_shaped_weights_conv_(m, ls, ks_half)\n        else:\n            m.reset_parameters()\n\n    elif isinstance(m, MirroredConv1d):\n        if kwargs.get(\"bell\", False):\n            # Initialize the kernel weights with a shifted of shape exp(-x^2 / sigma^2).\n            # The biases are left unchanged (does not exist by default).\n            ks = m.weight.data.size(2)  # ks_mirr = ceil(ks_conv1d / 2)\n            ls = torch.linspace(ks, 0, ks)  # descending\n            apply_bell_shaped_weights_conv_(m, ls, ks)\n        else:\n            m.reset_parameters()\n\n    elif isinstance(m, IndependentNonlinearitiesLayer):\n        # Initialize the network's parameters according to a normal distribution.\n        for tensor in (m.weight, m.bias):\n            if tensor is not None:\n                nn.init.normal_(tensor, std=1.0 / math.sqrt(tensor.nelement()))\n\n    elif isinstance(m, nn.Linear):\n        if kwargs.get(\"self_centric_init\", False):\n            m.weight.data.fill_(-0.5)  # inhibit others\n            for i in range(m.weight.data.size(0)):\n                m.weight.data[i, i] = 1.0  # excite self\n</code></pre>"},{"location":"reference/custom_types/","title":"custom_types","text":""},{"location":"reference/neural_fields/","title":"neural_fields","text":""},{"location":"reference/neural_fields/#neuralfields.neural_fields.NeuralField","title":"<code>NeuralField(input_size, hidden_size, output_size=None, input_embedding=None, output_embedding=None, activation_nonlin=torch.sigmoid, mirrored_conv_weights=True, conv_kernel_size=None, conv_padding_mode='circular', conv_out_channels=1, conv_pooling_norm=1, tau_init=10, tau_learnable=True, kappa_init=0, kappa_learnable=True, potentials_init=None, init_param_kwargs=None, device='cpu', dtype=None)</code>","text":"<p>         Bases: <code>PotentialBased</code></p> <p>A potential-based recurrent neural network according to [Amari, 1977].</p> See Also <p>[Amari, 1977] S.-I. Amari, \"Dynamics of Pattern Formation in Lateral-Inhibition Type Neural Fields\", Biological Cybernetics, 1977.</p> <pre><code>hidden_size: Number of neurons with potential in the (single) hidden layer.\noutput_size: Number of output dimensions. By default, the number of outputs is equal to the number of\n    hidden neurons.\ninput_embedding: Optional (custom) [Module][torch.nn.Module] to extract features from the inputs.\n    This module must transform the inputs such that the dimensionality matches the number of\n    neurons of the neural field, i.e., `hidden_size`. By default, a [linear layer][torch.nn.Linear]\n    without biases is used.\noutput_embedding: Optional (custom) [Module][torch.nn.Module] to compute the outputs from the activations.\n    This module must map the activations of shape (`hidden_size`,) to the outputs of shape (`output_size`,)\n    By default, a [linear layer][torch.nn.Linear] without biases is used.\nactivation_nonlin: Nonlinearity used to compute the activations from the potential levels.\nmirrored_conv_weights: If `True`, re-use weights for the second half of the kernel to create a\n    symmetric convolution kernel.\nconv_kernel_size: Size of the kernel for the 1-dim convolution along the potential-based neurons.\nconv_padding_mode: Padding mode forwarded to [Conv1d][torch.nn.Conv1d], options are \"circular\",\n    \"reflect\", or \"zeros\".\nconv_out_channels: Number of filter for the 1-dim convolution along the potential-based neurons.\nconv_pooling_norm: Norm type of the [torch.nn.LPPool1d][] pooling layer applied after the convolution.\n    Unlike in typical scenarios, here the pooling is performed over the channel dimension. Thus, varying\n    `conv_pooling_norm` only has an effect if `conv_out_channels &gt; 1`.\ntau_init: Initial value for the shared time constant of the potentials.\ntau_learnable: Whether the time constant is a learnable parameter or fixed.\nkappa_init: Initial value for the cubic decay, pass 0 to disable the cubic decay.\nkappa_learnable: Whether the cubic decay is a learnable parameter or fixed.\npotentials_init: Initial for the potentials, i.e., the network's hidden state.\ninit_param_kwargs: Additional keyword arguments for the policy parameter initialization.\ndevice: Device to move this module to (after initialization).\ndtype: Data type forwarded to the initializer of [Conv1d][torch.nn.Conv1d].\n</code></pre> Source code in <code>neuralfields/neural_fields.py</code> <pre><code>def __init__(\n    self,\n    input_size: int,\n    hidden_size: int,\n    output_size: Optional[int] = None,\n    input_embedding: Optional[nn.Module] = None,\n    output_embedding: Optional[nn.Module] = None,\n    activation_nonlin: Union[ActivationFunction, Sequence[ActivationFunction]] = torch.sigmoid,\n    mirrored_conv_weights: bool = True,\n    conv_kernel_size: Optional[int] = None,\n    conv_padding_mode: str = \"circular\",\n    conv_out_channels: int = 1,\n    conv_pooling_norm: int = 1,\n    tau_init: Union[float, int] = 10,\n    tau_learnable: bool = True,\n    kappa_init: Union[float, int] = 0,\n    kappa_learnable: bool = True,\n    potentials_init: Optional[torch.Tensor] = None,\n    init_param_kwargs: Optional[dict] = None,\n    device: Union[str, torch.device] = \"cpu\",\n    dtype: Optional[torch.dtype] = None,\n):\n\"\"\"\n    Args:\n        input_size: Number of input dimensions.\n        hidden_size: Number of neurons with potential in the (single) hidden layer.\n        output_size: Number of output dimensions. By default, the number of outputs is equal to the number of\n            hidden neurons.\n        input_embedding: Optional (custom) [Module][torch.nn.Module] to extract features from the inputs.\n            This module must transform the inputs such that the dimensionality matches the number of\n            neurons of the neural field, i.e., `hidden_size`. By default, a [linear layer][torch.nn.Linear]\n            without biases is used.\n        output_embedding: Optional (custom) [Module][torch.nn.Module] to compute the outputs from the activations.\n            This module must map the activations of shape (`hidden_size`,) to the outputs of shape (`output_size`,)\n            By default, a [linear layer][torch.nn.Linear] without biases is used.\n        activation_nonlin: Nonlinearity used to compute the activations from the potential levels.\n        mirrored_conv_weights: If `True`, re-use weights for the second half of the kernel to create a\n            symmetric convolution kernel.\n        conv_kernel_size: Size of the kernel for the 1-dim convolution along the potential-based neurons.\n        conv_padding_mode: Padding mode forwarded to [Conv1d][torch.nn.Conv1d], options are \"circular\",\n            \"reflect\", or \"zeros\".\n        conv_out_channels: Number of filter for the 1-dim convolution along the potential-based neurons.\n        conv_pooling_norm: Norm type of the [torch.nn.LPPool1d][] pooling layer applied after the convolution.\n            Unlike in typical scenarios, here the pooling is performed over the channel dimension. Thus, varying\n            `conv_pooling_norm` only has an effect if `conv_out_channels &gt; 1`.\n        tau_init: Initial value for the shared time constant of the potentials.\n        tau_learnable: Whether the time constant is a learnable parameter or fixed.\n        kappa_init: Initial value for the cubic decay, pass 0 to disable the cubic decay.\n        kappa_learnable: Whether the cubic decay is a learnable parameter or fixed.\n        potentials_init: Initial for the potentials, i.e., the network's hidden state.\n        init_param_kwargs: Additional keyword arguments for the policy parameter initialization.\n        device: Device to move this module to (after initialization).\n        dtype: Data type forwarded to the initializer of [Conv1d][torch.nn.Conv1d].\n    \"\"\"\n    if hidden_size &lt; 2:\n        raise ValueError(\"The humber of hidden neurons hidden_size must be at least 2!\")\n    if conv_kernel_size is None:\n        conv_kernel_size = hidden_size\n    if conv_padding_mode not in [\"circular\", \"reflect\", \"zeros\"]:\n        raise ValueError(\"The conv_padding_mode must be either 'circular', 'reflect', or 'zeros'!\")\n    if not callable(activation_nonlin):\n        raise ValueError(\"The activation function activation_nonlin must be a callable!\")\n    init_param_kwargs = init_param_kwargs if init_param_kwargs is not None else dict()\n\n    # Set the multiprocessing start method to spawn, since PyTorch is using the GPU for convolutions if it can.\n    if mp.get_start_method(allow_none=True) != \"spawn\":\n        mp.set_start_method(\"spawn\", force=True)\n\n    # Create the common layers and parameters.\n    super().__init__(\n        input_size=input_size,\n        hidden_size=hidden_size,\n        output_size=output_size,\n        activation_nonlin=activation_nonlin,\n        tau_init=tau_init,\n        tau_learnable=tau_learnable,\n        kappa_init=kappa_init,\n        kappa_learnable=kappa_learnable,\n        potentials_init=potentials_init,\n        input_embedding=input_embedding,\n        output_embedding=output_embedding,\n    )\n\n    # Create the custom convolution layer that models the interconnection of neurons, i.e., their potentials.\n    self.mirrored_conv_weights = mirrored_conv_weights\n    conv1d_class = MirroredConv1d if self.mirrored_conv_weights else nn.Conv1d\n    self.conv_layer = conv1d_class(\n        in_channels=1,  # treat potentials as a time series of values (convolutions is over the \"time\" axis)\n        out_channels=conv_out_channels,\n        kernel_size=conv_kernel_size,\n        padding_mode=conv_padding_mode,\n        padding=\"same\",  # to preserve the length od the output sequence\n        bias=False,\n        stride=1,\n        dilation=1,\n        groups=1,\n        # device=device,\n        dtype=dtype,\n    )\n    init_param_(self.conv_layer, **init_param_kwargs)\n\n    # Create a pooling layer that reduced all output channels to one.\n    self.conv_pooling_layer = torch.nn.LPPool1d(\n        conv_pooling_norm, kernel_size=conv_out_channels, stride=conv_out_channels\n    )\n\n    # Create the layer that converts the activations of the previous time step into potentials.\n    self.potentials_to_activations = IndependentNonlinearitiesLayer(\n        self._hidden_size, activation_nonlin, bias=True, weight=True\n    )\n\n    # Create the custom output embedding layer that combines the activations.\n    self.output_embedding = nn.Linear(self._hidden_size, self.output_size, bias=False)\n\n    # Move the complete model to the given device.\n    self.to(device=device)\n</code></pre>"},{"location":"reference/neural_fields/#neuralfields.neural_fields.NeuralField.potentials_dot","title":"<code>potentials_dot(potentials, stimuli)</code>","text":"<p>Compute the derivative of the neurons' potentials w.r.t. time.</p> <p>\\(/tau /dot{u} = s + h - u + /kappa (h - u)^3, /quad /text{with} s = s_{int} + s_{ext} = W*o + /int{w(u, v) f(u) dv}\\) with the potentials \\(u\\), the combined stimuli \\(s\\), the resting level \\(h\\), and the cubic decay \\(\\kappa\\).</p> <p>Parameters:</p> Name Type Description Default <code>potentials</code> <code>torch.Tensor</code> <p>Potential values at the current point in time, of shape <code>(hidden_size,)</code>.</p> required <code>stimuli</code> <code>torch.Tensor</code> <p>Sum of external and internal stimuli at the current point in time, of shape <code>(hidden_size,)</code>.</p> required <p>Returns:</p> Type Description <code>torch.Tensor</code> <p>Time derivative of the potentials \\(\\frac{dp}{dt}\\), of shape <code>(hidden_size,)</code>.</p> Source code in <code>neuralfields/neural_fields.py</code> <pre><code>def potentials_dot(self, potentials: torch.Tensor, stimuli: torch.Tensor) -&gt; torch.Tensor:\nr\"\"\"Compute the derivative of the neurons' potentials w.r.t. time.\n\n     $/tau /dot{u} = s + h - u + /kappa (h - u)^3,\n    /quad /text{with} s = s_{int} + s_{ext} = W*o + /int{w(u, v) f(u) dv}$\n    with the potentials $u$, the combined stimuli $s$, the resting level $h$, and the cubic decay $\\kappa$.\n\n    Args:\n        potentials: Potential values at the current point in time, of shape `(hidden_size,)`.\n        stimuli: Sum of external and internal stimuli at the current point in time, of shape `(hidden_size,)`.\n\n    Returns:\n        Time derivative of the potentials $\\frac{dp}{dt}$, of shape `(hidden_size,)`.\n    \"\"\"\n    rhs = stimuli + self.resting_level - potentials + self.kappa * torch.pow(self.resting_level - potentials, 3)\n    return rhs / self.tau\n</code></pre>"},{"location":"reference/potential_based/","title":"potential_based","text":""},{"location":"reference/potential_based/#neuralfields.potential_based.PotentialBased","title":"<code>PotentialBased(input_size, hidden_size, activation_nonlin, tau_init, tau_learnable, kappa_init, kappa_learnable, potentials_init=None, output_size=None, input_embedding=None, output_embedding=None)</code>","text":"<p>         Bases: <code>nn.Module</code>, <code>ABC</code></p> <p>Base class for all potential-based recurrent neutral networks.</p> <pre><code>hidden_size: Number of neurons with potential per hidden layer. For all use cases conceived at this point,\n    we only use one recurrent layer. However, there is the possibility to extend the networks to multiple\n    potential-based layers.\nactivation_nonlin: Nonlinearity used to compute the activations from the potential levels.\ntau_init: Initial value for the shared time constant of the potentials.\ntau_learnable: Whether the time constant is a learnable parameter or fixed.\nkappa_init: Initial value for the cubic decay, pass 0 to disable the cubic decay.\nkappa_learnable: Whether the cubic decay is a learnable parameter or fixed.\npotentials_init: Initial for the potentials, i.e., the network's hidden state.\noutput_size: Number of output dimensions. By default, the number of outputs is equal to the number of\n    hidden neurons.\ninput_embedding: Optional (custom) [Module][torch.nn.Module] to extract features from the inputs.\n    This module must transform the inputs such that the dimensionality matches the number of\n    neurons of the neural field, i.e., `hidden_size`. By default, a [linear layer][torch.nn.Linear]\n    without biases is used.\noutput_embedding: Optional (custom) [Module][torch.nn.Module] to compute the outputs from the activations.\n    This module must map the activations of shape (`hidden_size`,) to the outputs of shape (`output_size`,)\n    By default, a [linear layer][torch.nn.Linear] without biases is used.\n</code></pre> Source code in <code>neuralfields/potential_based.py</code> <pre><code>def __init__(\n    self,\n    input_size: int,\n    hidden_size: int,\n    activation_nonlin: Union[ActivationFunction, Sequence[ActivationFunction]],\n    tau_init: Union[float, int],\n    tau_learnable: bool,\n    kappa_init: Union[float, int],\n    kappa_learnable: bool,\n    potentials_init: Optional[torch.Tensor] = None,\n    output_size: Optional[int] = None,\n    input_embedding: Optional[nn.Module] = None,\n    output_embedding: Optional[nn.Module] = None,\n):\n\"\"\"\n    Args:\n        input_size: Number of input dimensions.\n        hidden_size: Number of neurons with potential per hidden layer. For all use cases conceived at this point,\n            we only use one recurrent layer. However, there is the possibility to extend the networks to multiple\n            potential-based layers.\n        activation_nonlin: Nonlinearity used to compute the activations from the potential levels.\n        tau_init: Initial value for the shared time constant of the potentials.\n        tau_learnable: Whether the time constant is a learnable parameter or fixed.\n        kappa_init: Initial value for the cubic decay, pass 0 to disable the cubic decay.\n        kappa_learnable: Whether the cubic decay is a learnable parameter or fixed.\n        potentials_init: Initial for the potentials, i.e., the network's hidden state.\n        output_size: Number of output dimensions. By default, the number of outputs is equal to the number of\n            hidden neurons.\n        input_embedding: Optional (custom) [Module][torch.nn.Module] to extract features from the inputs.\n            This module must transform the inputs such that the dimensionality matches the number of\n            neurons of the neural field, i.e., `hidden_size`. By default, a [linear layer][torch.nn.Linear]\n            without biases is used.\n        output_embedding: Optional (custom) [Module][torch.nn.Module] to compute the outputs from the activations.\n            This module must map the activations of shape (`hidden_size`,) to the outputs of shape (`output_size`,)\n            By default, a [linear layer][torch.nn.Linear] without biases is used.\n    \"\"\"\n    # Call torch.nn.Module's constructor.\n    super().__init__()\n\n    # For all use cases conceived at this point, we only use one recurrent layer. However, this variable still\n    # exists in case somebody in the future wants to try multiple potential-based layers. It will require more\n    # changes than increasing this number.\n    self.num_recurrent_layers = 1\n\n    self.input_size = input_size\n    self._hidden_size = hidden_size // self.num_recurrent_layers  # hidden size per layer\n    self.output_size = self._hidden_size if output_size is None else output_size\n    self._stimuli_external = torch.zeros(self.hidden_size)\n    self._stimuli_internal = torch.zeros(self.hidden_size)\n\n    # Create the common layers.\n    self.input_embedding = input_embedding or nn.Linear(self.input_size, self._hidden_size, bias=False)\n    self.output_embedding = output_embedding or nn.Linear(self._hidden_size, self.output_size, bias=False)\n\n    # Initialize the values of the potentials.\n    if potentials_init is not None:\n        self._potentials_init = potentials_init.detach().clone()\n    else:\n        if activation_nonlin is torch.sigmoid:\n            self._potentials_init = -7 * torch.ones(1, self.hidden_size)\n        else:\n            self._potentials_init = torch.zeros(1, self.hidden_size)\n\n    # Initialize the potentials' resting level, i.e., the asymptotic level without stimuli.\n    self.resting_level = nn.Parameter(torch.randn(self.hidden_size), requires_grad=True)\n\n    # Initialize the potential dynamics' time constant.\n    self.tau_learnable = tau_learnable\n    self._log_tau_init = torch.log(torch.as_tensor(tau_init, dtype=torch.get_default_dtype()).reshape(-1))\n    if self.tau_learnable:\n        self._log_tau = nn.Parameter(self._log_tau_init, requires_grad=True)\n    else:\n        self._log_tau = self._log_tau_init\n\n    # Initialize the potential dynamics' cubic decay.\n    self.kappa_learnable = kappa_learnable\n    self._log_kappa_init = torch.log(torch.as_tensor(kappa_init, dtype=torch.get_default_dtype()).reshape(-1))\n    if self.kappa_learnable:\n        self._log_kappa = nn.Parameter(self._log_kappa_init, requires_grad=True)\n    else:\n        self._log_kappa = self._log_kappa_init\n</code></pre>"},{"location":"reference/potential_based/#neuralfields.potential_based.PotentialBased.device","title":"<code>device: torch.device</code>  <code>property</code>","text":"<p>Get the device this model is located on. This assumes that all parts are located on the same device.</p>"},{"location":"reference/potential_based/#neuralfields.potential_based.PotentialBased.hidden_size","title":"<code>hidden_size: int</code>  <code>property</code>","text":"<p>Get the number of neurons in the neural field layer, i.e., the ones with the in-/exhibition dynamics.</p>"},{"location":"reference/potential_based/#neuralfields.potential_based.PotentialBased.kappa","title":"<code>kappa: Union[torch.Tensor, nn.Parameter]</code>  <code>property</code>","text":"<p>Get the cubic decay parameter \\(\\kappa\\).</p>"},{"location":"reference/potential_based/#neuralfields.potential_based.PotentialBased.param_values","title":"<code>param_values: torch.Tensor</code>  <code>property</code> <code>writable</code>","text":"<p>Get the module's parameters as a 1-dimensional array. The values are copied, thus modifying the return value does not propagate back to the module parameters.</p>"},{"location":"reference/potential_based/#neuralfields.potential_based.PotentialBased.stimuli_external","title":"<code>stimuli_external: torch.Tensor</code>  <code>property</code>","text":"<p>Get the neurons' external stimuli, resulting from the current inputs. This property is useful for recording during a simulation / rollout.</p>"},{"location":"reference/potential_based/#neuralfields.potential_based.PotentialBased.stimuli_internal","title":"<code>stimuli_internal: torch.Tensor</code>  <code>property</code>","text":"<p>Get the neurons' internal stimuli, resulting from the previous activations of the neurons. This property is useful for recording during a simulation / rollout.</p>"},{"location":"reference/potential_based/#neuralfields.potential_based.PotentialBased.tau","title":"<code>tau: Union[torch.Tensor, nn.Parameter]</code>  <code>property</code>","text":"<p>Get the timescale parameter, called \\(\\tau\\) in the original paper [Amari_77].</p>"},{"location":"reference/potential_based/#neuralfields.potential_based.PotentialBased.forward","title":"<code>forward(inputs, hidden=None)</code>","text":"<p>Compute the external and internal stimuli, advance the potential dynamics for one time step, and return the model's output for several time steps in a row.</p> <p>This method essentially calls forward_one_step several times in a row.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>torch.Tensor</code> <p>Inputs of shape <code>(batch_size, num_steps, dim_input)</code> to evaluate the network on.</p> required <code>hidden</code> <code>Optional[torch.Tensor]</code> <p>Initial values of the hidden states, i.e., the potentials. By default, the network initialized the hidden state to be all zeros. However, via this argument one can set a specific initial value for the potentials. Depending on the shape of <code>inputs</code>, <code>hidden</code> is of shape <code>(hidden_size,)</code> if the input was not batched, else of shape <code>(batch_size, hidden_size)</code>.</p> <code>None</code> <p>Returns:</p> Type Description <code>torch.Tensor</code> <p>The outputs, i.e., the (linearly combined) activations, and all intermediate potential values, both of</p> <code>torch.Tensor</code> <p>shape <code>(batch_size, num_steps, dim_output)</code>.</p> Source code in <code>neuralfields/potential_based.py</code> <pre><code>def forward(self, inputs: torch.Tensor, hidden: Optional[torch.Tensor] = None) -&gt; Tuple[torch.Tensor, torch.Tensor]:\n\"\"\"Compute the external and internal stimuli, advance the potential dynamics for one time step, and return\n    the model's output for several time steps in a row.\n\n    This method essentially calls [forward_one_step][neuralfields.PotentialBased.forward_one_step] several times\n    in a row.\n\n    Args:\n        inputs: Inputs of shape `(batch_size, num_steps, dim_input)` to evaluate the network on.\n        hidden: Initial values of the hidden states, i.e., the potentials. By default, the network initialized\n            the hidden state to be all zeros. However, via this argument one can set a specific initial value\n            for the potentials. Depending on the shape of `inputs`, `hidden` is of shape `(hidden_size,)` if\n            the input was not batched, else of shape `(batch_size, hidden_size)`.\n\n    Returns:\n        The outputs, i.e., the (linearly combined) activations, and all intermediate potential values, both of\n        shape `(batch_size, num_steps, dim_output)`.\n    \"\"\"\n    # Bring the sequence of inputs into the shape (batch_size, num_steps, dim_input).\n    batch_size = PotentialBased._infer_batch_size(inputs)\n    inputs = inputs.view(batch_size, -1, self.input_size)  # moved to the desired device by forward_one_step() later\n\n    # If given use the hidden tensor, i.e., the potentials of the last step, else initialize them.\n    hidden = self.init_hidden(batch_size, hidden)  # moved to the desired device by forward_one_step() later\n\n    # Iterate over the time dimension. Do this in parallel for all batched which are still along the 1st dimension.\n    inputs = inputs.permute(1, 0, 2)  # move time to first dimension for easy iterating\n    outputs_all = []\n    hidden_all = []\n    for inp in inputs:\n        outputs, hidden_next = self.forward_one_step(inp, hidden)\n        hidden = hidden_next.clone()\n        outputs_all.append(outputs)\n        hidden_all.append(hidden_next)\n\n    # Return the outputs and hidden states, both stacked along the time dimension.\n    return torch.stack(outputs_all, dim=1), torch.stack(hidden_all, dim=1)\n</code></pre>"},{"location":"reference/potential_based/#neuralfields.potential_based.PotentialBased.forward_one_step","title":"<code>forward_one_step(inputs, hidden=None)</code>  <code>abstractmethod</code>","text":"<p>Compute the external and internal stimuli, advance the potential dynamics for one time step, and return the model's output.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>torch.Tensor</code> <p>Inputs of the current time step, of shape <code>(input_size,)</code>, or <code>(batch_size, input_size)</code>.</p> required <code>hidden</code> <code>Optional[torch.Tensor]</code> <p>Hidden state which are for the model in this package the potentials, of shape <code>(hidden_size,)</code>, or <code>(batch_size, input_size)</code>. Pass <code>None</code> to leave the initialization to the network which uses init_hidden is called.</p> <code>None</code> <p>Returns:</p> Type Description <code>torch.Tensor</code> <p>The outputs, i.e., the (linearly combined) activations, and the most recent potential values, both of shape</p> <code>torch.Tensor</code> <p><code>(batch_size, input_size)</code>.</p> Source code in <code>neuralfields/potential_based.py</code> <pre><code>@abstractmethod\ndef forward_one_step(\n    self, inputs: torch.Tensor, hidden: Optional[torch.Tensor] = None\n) -&gt; Tuple[torch.Tensor, torch.Tensor]:\n\"\"\"Compute the external and internal stimuli, advance the potential dynamics for one time step, and return\n    the model's output.\n\n    Args:\n        inputs: Inputs of the current time step, of shape `(input_size,)`, or `(batch_size, input_size)`.\n        hidden: Hidden state which are for the model in this package the potentials, of shape `(hidden_size,)`, or\n            `(batch_size, input_size)`. Pass `None` to leave the initialization to the network which uses\n            [init_hidden][neuralfields.PotentialBased.init_hidden] is called.\n\n    Returns:\n        The outputs, i.e., the (linearly combined) activations, and the most recent potential values, both of shape\n        `(batch_size, input_size)`.\n    \"\"\"\n</code></pre>"},{"location":"reference/potential_based/#neuralfields.potential_based.PotentialBased.init_hidden","title":"<code>init_hidden(batch_size=None, potentials_init=None)</code>","text":"<p>Provide initial values for the hidden parameters. This usually is a zero tensor.</p> <p>Parameters:</p> Name Type Description Default <code>batch_size</code> <code>Optional[int]</code> <p>Number of batches, i.e., states to track in parallel.</p> <code>None</code> <code>potentials_init</code> <code>Optional[torch.Tensor]</code> <p>Initial values for the potentials to override the networks default values with.</p> <code>None</code> <p>Returns:</p> Type Description <code>Union[torch.Tensor, torch.nn.Parameter]</code> <p>Tensor of shape <code>(hidden_size,)</code> if <code>hidden</code> was not batched, else of shape <code>(batch_size, hidden_size)</code>.</p> Source code in <code>neuralfields/potential_based.py</code> <pre><code>def init_hidden(\n    self, batch_size: Optional[int] = None, potentials_init: Optional[torch.Tensor] = None\n) -&gt; Union[torch.Tensor, torch.nn.Parameter]:\n\"\"\"Provide initial values for the hidden parameters. This usually is a zero tensor.\n\n    Args:\n        batch_size: Number of batches, i.e., states to track in parallel.\n        potentials_init: Initial values for the potentials to override the networks default values with.\n\n    Returns:\n        Tensor of shape `(hidden_size,)` if `hidden` was not batched, else of shape `(batch_size, hidden_size)`.\n    \"\"\"\n    if potentials_init is None:\n        if batch_size is None:\n            return self._potentials_init.view(-1)\n        return self._potentials_init.repeat(batch_size, 1)\n\n    return potentials_init.to(device=self.device)\n</code></pre>"},{"location":"reference/potential_based/#neuralfields.potential_based.PotentialBased.potentials_dot","title":"<code>potentials_dot(potentials, stimuli)</code>  <code>abstractmethod</code>","text":"<p>Compute the derivative of the neurons' potentials w.r.t. time.</p> <p>Parameters:</p> Name Type Description Default <code>potentials</code> <code>torch.Tensor</code> <p>Potential values at the current point in time, of shape <code>(hidden_size,)</code>.</p> required <code>stimuli</code> <code>torch.Tensor</code> <p>Sum of external and internal stimuli at the current point in time, of shape <code>(hidden_size,)</code>.</p> required <p>Returns:</p> Type Description <code>torch.Tensor</code> <p>Time derivative of the potentials $\frac{dp}{dt}$, of shape <code>(hidden_size,)</code>.</p> Source code in <code>neuralfields/potential_based.py</code> <pre><code>@abstractmethod\ndef potentials_dot(self, potentials: torch.Tensor, stimuli: torch.Tensor) -&gt; torch.Tensor:\n\"\"\"Compute the derivative of the neurons' potentials w.r.t. time.\n\n    Args:\n        potentials: Potential values at the current point in time, of shape `(hidden_size,)`.\n        stimuli: Sum of external and internal stimuli at the current point in time, of shape `(hidden_size,)`.\n\n    Returns:\n        Time derivative of the potentials $\\frac{dp}{dt}$, of shape `(hidden_size,)`.\n    \"\"\"\n</code></pre>"},{"location":"reference/simple_neural_fields/","title":"simple_neural_fields","text":""},{"location":"reference/simple_neural_fields/#neuralfields.simple_neural_fields.SimpleNeuralField","title":"<code>SimpleNeuralField(input_size, output_size, potentials_dyn_fcn, input_embedding=None, output_embedding=None, activation_nonlin=torch.sigmoid, tau_init=10.0, tau_learnable=True, kappa_init=0.001, kappa_learnable=True, capacity_learnable=True, potentials_init=None, init_param_kwargs=None, device='cpu')</code>","text":"<p>         Bases: <code>PotentialBased</code></p> <p>A simplified version of Amari's potential-based recurrent neural network, without the convolution over time.</p> See Also <p>[Luksch et al., 2012] T. Luksch, M. Gineger, M. M\u00fchlig, T. Yoshiike, \"Adaptive Movement Sequences and Predictive Decisions based on Hierarchical Dynamical Systems\", International Conference on Intelligent Robots and Systems, 2012.</p> <pre><code>output_size: Number of output dimensions. For this simplified neural fields model, the number of outputs\n    is equal to the number of neurons in the (single) hidden layer.\ninput_embedding: Optional (custom) [Module][torch.nn.Module] to extract features from the inputs.\n    This module must transform the inputs such that the dimensionality matches the number of\n    neurons of the neural field, i.e., `hidden_size`. By default, a [linear layer][torch.nn.Linear]\n    without biases is used.\noutput_embedding: Optional (custom) [Module][torch.nn.Module] to compute the outputs from the activations.\n    This module must map the activations of shape (`hidden_size`,) to the outputs of shape (`output_size`,)\n    By default, a [linear layer][torch.nn.Linear] without biases is used.\nactivation_nonlin: Nonlinearity used to compute the activations from the potential levels.\ntau_init: Initial value for the shared time constant of the potentials.\ntau_learnable: Whether the time constant is a learnable parameter or fixed.\nkappa_init: Initial value for the cubic decay, pass 0 to disable the cubic decay.\nkappa_learnable: Whether the cubic decay is a learnable parameter or fixed.\ncapacity_learnable: Whether the capacity is a learnable parameter or fixed.\npotentials_init: Initial for the potentials, i.e., the network's hidden state.\ninit_param_kwargs: Additional keyword arguments for the policy parameter initialization. For example,\n    `self_centric_init=True` to initialize the interaction between neurons such that they inhibit the\n    others and excite themselves.\ndevice: Device to move this module to (after initialization).\n</code></pre> Source code in <code>neuralfields/simple_neural_fields.py</code> <pre><code>def __init__(\n    self,\n    input_size: int,\n    output_size: int,\n    potentials_dyn_fcn: PotentialsDynamicsType,\n    input_embedding: Optional[nn.Module] = None,\n    output_embedding: Optional[nn.Module] = None,\n    activation_nonlin: Union[ActivationFunction, Sequence[ActivationFunction]] = torch.sigmoid,\n    tau_init: Union[float, int] = 10.0,\n    tau_learnable: bool = True,\n    kappa_init: Union[float, int] = 1e-3,\n    kappa_learnable: bool = True,\n    capacity_learnable: bool = True,\n    potentials_init: Optional[torch.Tensor] = None,\n    init_param_kwargs: Optional[dict] = None,\n    device: Union[str, torch.device] = \"cpu\",\n):\n\"\"\"\n    Args:\n        input_size: Number of input dimensions.\n        output_size: Number of output dimensions. For this simplified neural fields model, the number of outputs\n            is equal to the number of neurons in the (single) hidden layer.\n        input_embedding: Optional (custom) [Module][torch.nn.Module] to extract features from the inputs.\n            This module must transform the inputs such that the dimensionality matches the number of\n            neurons of the neural field, i.e., `hidden_size`. By default, a [linear layer][torch.nn.Linear]\n            without biases is used.\n        output_embedding: Optional (custom) [Module][torch.nn.Module] to compute the outputs from the activations.\n            This module must map the activations of shape (`hidden_size`,) to the outputs of shape (`output_size`,)\n            By default, a [linear layer][torch.nn.Linear] without biases is used.\n        activation_nonlin: Nonlinearity used to compute the activations from the potential levels.\n        tau_init: Initial value for the shared time constant of the potentials.\n        tau_learnable: Whether the time constant is a learnable parameter or fixed.\n        kappa_init: Initial value for the cubic decay, pass 0 to disable the cubic decay.\n        kappa_learnable: Whether the cubic decay is a learnable parameter or fixed.\n        capacity_learnable: Whether the capacity is a learnable parameter or fixed.\n        potentials_init: Initial for the potentials, i.e., the network's hidden state.\n        init_param_kwargs: Additional keyword arguments for the policy parameter initialization. For example,\n            `self_centric_init=True` to initialize the interaction between neurons such that they inhibit the\n            others and excite themselves.\n        device: Device to move this module to (after initialization).\n    \"\"\"\n    init_param_kwargs = init_param_kwargs if init_param_kwargs is not None else dict()\n\n    # Create the common layers and parameters.\n    super().__init__(\n        input_size=input_size,\n        hidden_size=output_size,\n        output_size=output_size,\n        activation_nonlin=activation_nonlin,\n        tau_init=tau_init,\n        tau_learnable=tau_learnable,\n        kappa_init=kappa_init,\n        kappa_learnable=kappa_learnable,\n        potentials_init=potentials_init,\n        input_embedding=input_embedding,\n        output_embedding=output_embedding,\n    )\n\n    # Create the layer that converts the activations of the previous time step into potentials (internal stimulus).\n    # For this model, self._hidden_size equals output_size.\n    self.prev_activations_embedding = nn.Linear(self._hidden_size, self._hidden_size, bias=False)\n    init_param_(self.prev_activations_embedding, **init_param_kwargs)\n\n    # Create the layer that converts potentials into activations which are the outputs in this model.\n    # Scaling weights equals beta in eq (4) in [Luksch et al., 2012].\n    self.potentials_to_activations = IndependentNonlinearitiesLayer(\n        self._hidden_size, nonlin=activation_nonlin, bias=False, weight=True\n    )\n\n    # Potential dynamics.\n    self.potentials_dyn_fcn = potentials_dyn_fcn\n    self.capacity_learnable = capacity_learnable\n    if self.potentials_dyn_fcn in [pd_capacity_21, pd_capacity_21_abs, pd_capacity_32, pd_capacity_32_abs]:\n        if _is_iterable(activation_nonlin):\n            self._init_capacity(activation_nonlin[0])\n        else:\n            self._init_capacity(activation_nonlin)  # type: ignore[arg-type]\n    else:\n        self._log_capacity = None\n\n    # Initialize cubic decay and capacity if learnable.\n    if (self.potentials_dyn_fcn is pd_cubic) and self.kappa_learnable:\n        self._log_kappa.data = self._log_kappa_init\n    elif self.potentials_dyn_fcn in [pd_capacity_21, pd_capacity_21_abs, pd_capacity_32, pd_capacity_32_abs]:\n        self._log_capacity.data = self._log_capacity_init\n\n    # Move the complete model to the given device.\n    self.to(device=device)\n</code></pre>"},{"location":"reference/simple_neural_fields/#neuralfields.simple_neural_fields.SimpleNeuralField.capacity","title":"<code>capacity: Optional[torch.Tensor]</code>  <code>property</code>","text":"<p>Get the capacity parameter (exists for capacity-based dynamics functions), otherwise return <code>None</code>.</p>"},{"location":"reference/simple_neural_fields/#neuralfields.simple_neural_fields.SimpleNeuralField.potentials_dot","title":"<code>potentials_dot(potentials, stimuli)</code>","text":"<p>Compute the derivative of the neurons' potentials per time step.</p> <p>\\(/tau /dot{u} = f(u, s, h)\\) with the potentials \\(u\\), the combined stimuli \\(s\\), and the resting level \\(h\\).</p> <p>Parameters:</p> Name Type Description Default <code>potentials</code> <code>torch.Tensor</code> <p>Potential values at the current point in time, of shape <code>(hidden_size,)</code>.</p> required <code>stimuli</code> <code>torch.Tensor</code> <p>Sum of external and internal stimuli at the current point in time, of shape <code>(hidden_size,)</code>.</p> required <p>Returns:</p> Type Description <code>torch.Tensor</code> <p>Time derivative of the potentials \\(\\frac{dp}{dt}\\), of shape <code>(hidden_size,)</code>.</p> Source code in <code>neuralfields/simple_neural_fields.py</code> <pre><code>def potentials_dot(self, potentials: torch.Tensor, stimuli: torch.Tensor) -&gt; torch.Tensor:\nr\"\"\"Compute the derivative of the neurons' potentials per time step.\n\n    $/tau /dot{u} = f(u, s, h)$\n    with the potentials $u$, the combined stimuli $s$, and the resting level $h$.\n\n    Args:\n        potentials: Potential values at the current point in time, of shape `(hidden_size,)`.\n        stimuli: Sum of external and internal stimuli at the current point in time, of shape `(hidden_size,)`.\n\n    Returns:\n        Time derivative of the potentials $\\frac{dp}{dt}$, of shape `(hidden_size,)`.\n    \"\"\"\n    return self.potentials_dyn_fcn(potentials, stimuli, self.resting_level, self.tau, self.kappa, self.capacity)\n</code></pre>"},{"location":"reference/simple_neural_fields/#neuralfields.simple_neural_fields.pd_capacity_21","title":"<code>pd_capacity_21(p, s, h, tau, kappa, capacity)</code>","text":"<p>Capacity-based dynamics with 2 stable (\\(p=-C\\), \\(p=C\\)) and 1 unstable fix points (\\(p=0\\)) for \\(s=0\\)</p> <p>\\(\\tau \\dot{p} =  s - (h - p) (1 - \\frac{(h - p)^2}{C^2})\\)</p> Notes <p>This potential dynamics function is strongly recommended to be used with a tanh activation function.</p> <p>Parameters:</p> Name Type Description Default <code>p</code> <code>torch.Tensor</code> <p>Potential, higher values lead to higher activations.</p> required <code>s</code> <code>torch.Tensor</code> <p>Stimulus, higher values lead to larger changes of the potentials (depends on the dynamics function).</p> required <code>h</code> <code>torch.Tensor</code> <p>Resting level, a.k.a. constant offset.</p> required <code>tau</code> <code>torch.Tensor</code> <p>Time scaling factor, higher values lead to slower changes of the potentials (linear dependency).</p> required <code>kappa</code> <code>Optional[torch.Tensor]</code> <p>Cubic decay factor for a neuron's potential, ignored for this dynamics function.</p> required <code>capacity</code> <code>Optional[torch.Tensor]</code> <p>Capacity value of a neuron's potential.</p> required <p>Returns:</p> Type Description <code>torch.Tensor</code> <p>Time derivative of the potentials \\(\\frac{dp}{dt}\\).</p> Source code in <code>neuralfields/simple_neural_fields.py</code> <pre><code>def pd_capacity_21(\n    p: torch.Tensor,\n    s: torch.Tensor,\n    h: torch.Tensor,\n    tau: torch.Tensor,\n    kappa: Optional[torch.Tensor],\n    capacity: Optional[torch.Tensor],\n) -&gt; torch.Tensor:\nr\"\"\"Capacity-based dynamics with 2 stable ($p=-C$, $p=C$) and 1 unstable fix points ($p=0$) for $s=0$\n\n    $\\tau \\dot{p} =  s - (h - p) (1 - \\frac{(h - p)^2}{C^2})$\n\n    Notes:\n        This potential dynamics function is strongly recommended to be used with a [tanh][torch.tanh] activation\n        function.\n\n    Args:\n        p: Potential, higher values lead to higher activations.\n        s: Stimulus, higher values lead to larger changes of the potentials (depends on the dynamics function).\n        h: Resting level, a.k.a. constant offset.\n        tau: Time scaling factor, higher values lead to slower changes of the potentials (linear dependency).\n        kappa: Cubic decay factor for a neuron's potential, ignored for this dynamics function.\n        capacity: Capacity value of a neuron's potential.\n\n    Returns:\n        Time derivative of the potentials $\\frac{dp}{dt}$.\n    \"\"\"\n    _verify_tau(tau)\n    _verify_capacity(capacity)\n    return (s - (h - p) * (torch.ones_like(p) - (h - p) ** 2 / capacity**2)) / tau\n</code></pre>"},{"location":"reference/simple_neural_fields/#neuralfields.simple_neural_fields.pd_capacity_21_abs","title":"<code>pd_capacity_21_abs(p, s, h, tau, kappa, capacity)</code>","text":"<p>Capacity-based dynamics with 2 stable (\\(p=-C\\), \\(p=C\\)) and 1 unstable fix points (\\(p=0\\)) for \\(s=0\\)</p> <p>\\(\\tau \\dot{p} =  s - (h - p) (1 - \\frac{\\left| h - p \\right|}{C})\\)</p> <p>The \"absolute version\" of <code>pd_capacity_21</code> has a lower magnitude and a lower oder of the resulting polynomial.</p> Notes <p>This potential dynamics function is strongly recommended to be used with a tanh activation function.</p> <p>Parameters:</p> Name Type Description Default <code>p</code> <code>torch.Tensor</code> <p>Potential, higher values lead to higher activations.</p> required <code>s</code> <code>torch.Tensor</code> <p>Stimulus, higher values lead to larger changes of the potentials (depends on the dynamics function).</p> required <code>h</code> <code>torch.Tensor</code> <p>Resting level, a.k.a. constant offset.</p> required <code>tau</code> <code>torch.Tensor</code> <p>Time scaling factor, higher values lead to slower changes of the potentials (linear dependency).</p> required <code>kappa</code> <code>Optional[torch.Tensor]</code> <p>Cubic decay factor for a neuron's potential, ignored for this dynamics function.</p> required <code>capacity</code> <code>Optional[torch.Tensor]</code> <p>Capacity value of a neuron's potential.</p> required <p>Returns:</p> Type Description <code>torch.Tensor</code> <p>Time derivative of the potentials \\(\\frac{dp}{dt}\\).</p> Source code in <code>neuralfields/simple_neural_fields.py</code> <pre><code>def pd_capacity_21_abs(\n    p: torch.Tensor,\n    s: torch.Tensor,\n    h: torch.Tensor,\n    tau: torch.Tensor,\n    kappa: Optional[torch.Tensor],\n    capacity: Optional[torch.Tensor],\n) -&gt; torch.Tensor:\nr\"\"\"Capacity-based dynamics with 2 stable ($p=-C$, $p=C$) and 1 unstable fix points ($p=0$) for $s=0$\n\n    $\\tau \\dot{p} =  s - (h - p) (1 - \\frac{\\left| h - p \\right|}{C})$\n\n    The \"absolute version\" of `pd_capacity_21` has a lower magnitude and a lower oder of the resulting polynomial.\n\n    Notes:\n        This potential dynamics function is strongly recommended to be used with a [tanh][torch.tanh] activation\n        function.\n\n    Args:\n        p: Potential, higher values lead to higher activations.\n        s: Stimulus, higher values lead to larger changes of the potentials (depends on the dynamics function).\n        h: Resting level, a.k.a. constant offset.\n        tau: Time scaling factor, higher values lead to slower changes of the potentials (linear dependency).\n        kappa: Cubic decay factor for a neuron's potential, ignored for this dynamics function.\n        capacity: Capacity value of a neuron's potential.\n\n    Returns:\n        Time derivative of the potentials $\\frac{dp}{dt}$.\n    \"\"\"\n    _verify_tau(tau)\n    _verify_capacity(capacity)\n    return (s - (h - p) * (torch.ones_like(p) - torch.abs(h - p) / capacity)) / tau\n</code></pre>"},{"location":"reference/simple_neural_fields/#neuralfields.simple_neural_fields.pd_capacity_32","title":"<code>pd_capacity_32(p, s, h, tau, kappa, capacity)</code>","text":"<p>Capacity-based dynamics with 3 stable (\\(p=-C\\), \\(p=0\\), \\(p=C\\)) and 2 unstable fix points (\\(p=-C/2\\), \\(p=C/2\\)) for \\(s=0\\)</p> <p>\\(\\tau \\dot{p} =  s - (h - p) (1 - \\frac{(h - p)^2}{C^2}) (1 - \\frac{(2(h - p))^2}{C^2})\\)</p> Notes <p>This potential dynamics function is strongly recommended to be used with a tanh activation function.</p> <p>Parameters:</p> Name Type Description Default <code>p</code> <code>torch.Tensor</code> <p>Potential, higher values lead to higher activations.</p> required <code>s</code> <code>torch.Tensor</code> <p>Stimulus, higher values lead to larger changes of the potentials (depends on the dynamics function).</p> required <code>h</code> <code>torch.Tensor</code> <p>Resting level, a.k.a. constant offset.</p> required <code>tau</code> <code>torch.Tensor</code> <p>Time scaling factor, higher values lead to slower changes of the potentials (linear dependency).</p> required <code>kappa</code> <code>Optional[torch.Tensor]</code> <p>Cubic decay factor for a neuron's potential, ignored for this dynamics function.</p> required <code>capacity</code> <code>Optional[torch.Tensor]</code> <p>Capacity value of a neuron's potential.</p> required <p>Returns:</p> Type Description <code>torch.Tensor</code> <p>Time derivative of the potentials \\(\\frac{dp}{dt}\\).</p> Source code in <code>neuralfields/simple_neural_fields.py</code> <pre><code>def pd_capacity_32(\n    p: torch.Tensor,\n    s: torch.Tensor,\n    h: torch.Tensor,\n    tau: torch.Tensor,\n    kappa: Optional[torch.Tensor],\n    capacity: Optional[torch.Tensor],\n) -&gt; torch.Tensor:\nr\"\"\"Capacity-based dynamics with 3 stable ($p=-C$, $p=0$, $p=C$) and 2 unstable fix points ($p=-C/2$, $p=C/2$)\n    for $s=0$\n\n    $\\tau \\dot{p} =  s - (h - p) (1 - \\frac{(h - p)^2}{C^2}) (1 - \\frac{(2(h - p))^2}{C^2})$\n\n    Notes:\n        This potential dynamics function is strongly recommended to be used with a [tanh][torch.tanh] activation\n        function.\n\n    Args:\n        p: Potential, higher values lead to higher activations.\n        s: Stimulus, higher values lead to larger changes of the potentials (depends on the dynamics function).\n        h: Resting level, a.k.a. constant offset.\n        tau: Time scaling factor, higher values lead to slower changes of the potentials (linear dependency).\n        kappa: Cubic decay factor for a neuron's potential, ignored for this dynamics function.\n        capacity: Capacity value of a neuron's potential.\n\n    Returns:\n        Time derivative of the potentials $\\frac{dp}{dt}$.\n    \"\"\"\n    _verify_tau(tau)\n    _verify_capacity(capacity)\n    return (\n        s\n        + (h - p)\n        * (torch.ones_like(p) - (h - p) ** 2 / capacity**2)\n        * (torch.ones_like(p) - ((2 * (h - p)) ** 2 / capacity**2))\n    ) / tau\n</code></pre>"},{"location":"reference/simple_neural_fields/#neuralfields.simple_neural_fields.pd_capacity_32_abs","title":"<code>pd_capacity_32_abs(p, s, h, tau, kappa, capacity)</code>","text":"<p>Capacity-based dynamics with 3 stable (\\(p=-C\\), \\(p=0\\), \\(p=C\\)) and 2 unstable fix points (\\(p=-C/2\\), \\(p=C/2\\)) for \\(s=0\\).</p> <p>\\(\\tau \\dot{p} =  \\left( s + (h - p) (1 - \\frac{\\left| (h - p) \\right|}{C}) (1 - \\frac{2 \\left| (h - p) \\right|}{C}) \\right)\\)</p> <p>The \"absolute version\" of <code>pd_capacity_32</code> is less skewed due to a lower oder of the resulting polynomial.</p> Notes <p>This potential dynamics function is strongly recommended to be used with a tanh activation function.</p> <p>Parameters:</p> Name Type Description Default <code>p</code> <code>torch.Tensor</code> <p>Potential, higher values lead to higher activations.</p> required <code>s</code> <code>torch.Tensor</code> <p>Stimulus, higher values lead to larger changes of the potentials (depends on the dynamics function).</p> required <code>h</code> <code>torch.Tensor</code> <p>Resting level, a.k.a. constant offset.</p> required <code>tau</code> <code>torch.Tensor</code> <p>Time scaling factor, higher values lead to slower changes of the potentials (linear dependency).</p> required <code>kappa</code> <code>Optional[torch.Tensor]</code> <p>Cubic decay factor for a neuron's potential, ignored for this dynamics function.</p> required <code>capacity</code> <code>Optional[torch.Tensor]</code> <p>Capacity value of a neuron's potential.</p> required <p>Returns:</p> Type Description <code>torch.Tensor</code> <p>Time derivative of the potentials \\(\\frac{dp}{dt}\\).</p> Source code in <code>neuralfields/simple_neural_fields.py</code> <pre><code>def pd_capacity_32_abs(\n    p: torch.Tensor,\n    s: torch.Tensor,\n    h: torch.Tensor,\n    tau: torch.Tensor,\n    kappa: Optional[torch.Tensor],\n    capacity: Optional[torch.Tensor],\n) -&gt; torch.Tensor:\nr\"\"\"Capacity-based dynamics with 3 stable ($p=-C$, $p=0$, $p=C$) and 2 unstable fix points ($p=-C/2$, $p=C/2$)\n    for $s=0$.\n\n    $\\tau \\dot{p} =  \\left( s + (h - p) (1 - \\frac{\\left| (h - p) \\right|}{C})\n    (1 - \\frac{2 \\left| (h - p) \\right|}{C}) \\right)$\n\n    The \"absolute version\" of `pd_capacity_32` is less skewed due to a lower oder of the resulting polynomial.\n\n    Notes:\n        This potential dynamics function is strongly recommended to be used with a [tanh][torch.tanh] activation\n        function.\n\n    Args:\n        p: Potential, higher values lead to higher activations.\n        s: Stimulus, higher values lead to larger changes of the potentials (depends on the dynamics function).\n        h: Resting level, a.k.a. constant offset.\n        tau: Time scaling factor, higher values lead to slower changes of the potentials (linear dependency).\n        kappa: Cubic decay factor for a neuron's potential, ignored for this dynamics function.\n        capacity: Capacity value of a neuron's potential.\n\n    Returns:\n        Time derivative of the potentials $\\frac{dp}{dt}$.\n    \"\"\"\n    _verify_tau(tau)\n    _verify_capacity(capacity)\n    return (\n        s\n        + (h - p)\n        * (torch.ones_like(p) - torch.abs(h - p) / capacity)\n        * (torch.ones_like(p) - 2 * torch.abs(h - p) / capacity)\n    ) / tau\n</code></pre>"},{"location":"reference/simple_neural_fields/#neuralfields.simple_neural_fields.pd_cubic","title":"<code>pd_cubic(p, s, h, tau, kappa, capacity)</code>","text":"<p>Basic proportional dynamics with additional cubic decay.</p> <p>\\(\\tau \\dot{p} = s + h - p + \\kappa (h - p)^3\\)</p> Notes <p>This potential dynamics function is strongly recommended to be used with a sigmoid activation function.</p> <p>Parameters:</p> Name Type Description Default <code>p</code> <code>torch.Tensor</code> <p>Potential, higher values lead to higher activations.</p> required <code>s</code> <code>torch.Tensor</code> <p>Stimulus, higher values lead to larger changes of the potentials (depends on the dynamics function).</p> required <code>h</code> <code>torch.Tensor</code> <p>Resting level, a.k.a. constant offset.</p> required <code>tau</code> <code>torch.Tensor</code> <p>Time scaling factor, higher values lead to slower changes of the potentials (linear dependency).</p> required <code>kappa</code> <code>Optional[torch.Tensor]</code> <p>Cubic decay factor for a neuron's potential.</p> required <code>capacity</code> <code>Optional[torch.Tensor]</code> <p>Capacity value of a neuron's potential, ignored for this dynamics function.</p> required <p>Returns:</p> Type Description <code>torch.Tensor</code> <p>Time derivative of the potentials \\(\\frac{dp}{dt}\\).</p> Source code in <code>neuralfields/simple_neural_fields.py</code> <pre><code>def pd_cubic(\n    p: torch.Tensor,\n    s: torch.Tensor,\n    h: torch.Tensor,\n    tau: torch.Tensor,\n    kappa: Optional[torch.Tensor],\n    capacity: Optional[torch.Tensor],\n) -&gt; torch.Tensor:\nr\"\"\"Basic proportional dynamics with additional cubic decay.\n\n    $\\tau \\dot{p} = s + h - p + \\kappa (h - p)^3$\n\n    Notes:\n        This potential dynamics function is strongly recommended to be used with a [sigmoid][torch.sigmoid] activation\n        function.\n\n    Args:\n        p: Potential, higher values lead to higher activations.\n        s: Stimulus, higher values lead to larger changes of the potentials (depends on the dynamics function).\n        h: Resting level, a.k.a. constant offset.\n        tau: Time scaling factor, higher values lead to slower changes of the potentials (linear dependency).\n        kappa: Cubic decay factor for a neuron's potential.\n        capacity: Capacity value of a neuron's potential, ignored for this dynamics function.\n\n    Returns:\n        Time derivative of the potentials $\\frac{dp}{dt}$.\n    \"\"\"\n    _verify_tau(tau)\n    _verify_kappa(kappa)\n    return (s + h - p + kappa * torch.pow(h - p, 3)) / tau\n</code></pre>"},{"location":"reference/simple_neural_fields/#neuralfields.simple_neural_fields.pd_linear","title":"<code>pd_linear(p, s, h, tau, kappa, capacity)</code>","text":"<p>Basic proportional dynamics.</p> <p>\\(\\tau \\dot{p} = s - p\\)</p> Notes <p>This potential dynamics function is strongly recommended to be used with a sigmoid activation function.</p> <p>Parameters:</p> Name Type Description Default <code>p</code> <code>torch.Tensor</code> <p>Potential, higher values lead to higher activations.</p> required <code>s</code> <code>torch.Tensor</code> <p>Stimulus, higher values lead to larger changes of the potentials (depends on the dynamics function).</p> required <code>h</code> <code>torch.Tensor</code> <p>Resting level, a.k.a. constant offset.</p> required <code>tau</code> <code>torch.Tensor</code> <p>Time scaling factor, higher values lead to slower changes of the potentials (linear dependency).</p> required <code>kappa</code> <code>Optional[torch.Tensor]</code> <p>Cubic decay factor for a neuron's potential, ignored for this dynamics function.</p> required <code>capacity</code> <code>Optional[torch.Tensor]</code> <p>Capacity value of a neuron's potential, ignored for this dynamics function.</p> required <p>Returns:</p> Type Description <code>torch.Tensor</code> <p>Time derivative of the potentials \\(\\frac{dp}{dt}\\).</p> Source code in <code>neuralfields/simple_neural_fields.py</code> <pre><code>def pd_linear(\n    p: torch.Tensor,\n    s: torch.Tensor,\n    h: torch.Tensor,\n    tau: torch.Tensor,\n    kappa: Optional[torch.Tensor],\n    capacity: Optional[torch.Tensor],\n) -&gt; torch.Tensor:\nr\"\"\"Basic proportional dynamics.\n\n    $\\tau \\dot{p} = s - p$\n\n    Notes:\n        This potential dynamics function is strongly recommended to be used with a [sigmoid][torch.sigmoid] activation\n        function.\n\n    Args:\n        p: Potential, higher values lead to higher activations.\n        s: Stimulus, higher values lead to larger changes of the potentials (depends on the dynamics function).\n        h: Resting level, a.k.a. constant offset.\n        tau: Time scaling factor, higher values lead to slower changes of the potentials (linear dependency).\n        kappa: Cubic decay factor for a neuron's potential, ignored for this dynamics function.\n        capacity: Capacity value of a neuron's potential, ignored for this dynamics function.\n\n    Returns:\n        Time derivative of the potentials $\\frac{dp}{dt}$.\n    \"\"\"\n    _verify_tau(tau)\n    return (s + h - p) / tau\n</code></pre>"}]}